<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>个人计划表</title>
      <link href="2021/04/08/ge-ren-ji-hua-biao/"/>
      <url>2021/04/08/ge-ren-ji-hua-biao/</url>
      
        <content type="html"><![CDATA[<h3 id="2021-04-08-——-2021-07-08"><a href="#2021-04-08-——-2021-07-08" class="headerlink" title="2021-04-08 —— 2021-07-08"></a>2021-04-08 —— 2021-07-08</h3><p>TODO LIST</p><p>目的：提高理解和表达能力，更优秀的工作表现和工作产出。</p><ul><li><p>生活</p><p>  跑步</p><pre><code>   一次马拉松</code></pre><p>  阅读</p><pre><code>  《精准努力：刘媛媛的逆袭课》  </code></pre><p>  演讲</p><pre><code>  一次上台主题演讲</code></pre></li><li><p>工作</p><p>  算法策略</p><pre><code>  新玩家冷启动策略  优化推荐排序算法    指标预测</code></pre><p>  工作技能</p><pre><code>  模型融合</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> 人生感悟 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人生感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习推荐系统实战之Contextual-Bandit</title>
      <link href="2021/04/05/shen-du-xue-xi-tui-jian-xi-tong-shi-zhan-zhi-contextual-bandit/"/>
      <url>2021/04/05/shen-du-xue-xi-tui-jian-xi-tong-shi-zhan-zhi-contextual-bandit/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>个性化推荐系统通过调整服务内容（新闻、广告等）来为用户提供个性化服务。尽管当前阶段已经取得了一些进步，但是仍存在一些具有挑战性的问题。</p><ol><li>web服务要求动态更新推荐内容，传统的协同过滤方法不够灵活。</li><li>web服务要求推荐系统具有一定的模型学习效率并保证推荐实时性。</li></ol><p>论文把新闻推荐视为带有上下文信息的多臂老虎机问题。采用的一种有效的做法是根据用户和文章的上下文信息，选择收益最高的文章推荐给用户，同时基于用户点击反馈动态调整策略，从长远来看可以最大化用户的总点击次数。该算法在计算上高效且有很好的理论依据。实验证明加入上下文的Bandit算法比传统Bandit算法的点击率高了12.5%。当数据稀疏时，这个优势甚至更大。论文给出了LinUCB方法和几种传统bandit方法的离线实验对比。</p><p>传统的Bandit算法所存在的问题是没有利用不同臂(物品)之间的特征信息，当一个新物品加入内容库，我们只能从零开始积累用户的点击反馈。针对这种情况，识别物品之间的关联并具备一定的泛化能力就显得至关重要。例如，对于男青年而言，ipad相比养老计划对他更具吸引力。通过提取青年和iap特征，bandit算法可以将用户/文章泛化到下一个用户/文章，快速调整文章，解决冷启动问题。</p><p>LinUCB分为两种。</p><ul><li>线性不相交的disjoint LinUCB</li><li>混合相交的hybrid LinUCB</li></ul><h2 id="disjoint-LinUCB"><a href="#disjoint-LinUCB" class="headerlink" title="disjoint LinUCB"></a>disjoint LinUCB</h2><p>disjoint LinUCB的不同臂之间参数不共享。</p><p><img src="/2021/04/05/shen-du-xue-xi-tui-jian-xi-tong-shi-zhan-zhi-contextual-bandit/1.png" alt="1"></p><p>根据伪代码可以将算法流程分成两步，第一步取最大收益的臂，第二步根据反馈结果更新参数。</p><h5 id="选择最大收益的臂"><a href="#选择最大收益的臂" class="headerlink" title="选择最大收益的臂"></a>选择最大收益的臂</h5><p>一个臂进行m次推荐过程中，积累了m个样本，每一个样本的维度是d，则形成m×d维度的D矩阵。<br>其中m个样本含有用户点击/不点击的反馈信息，组成一个m×1的列向量C。</p><p>$D_{m×d}θ_{d×1}=C_{m×1}$</p><p>根据岭回归的矩阵解法，可以推导出</p><p>$θ_{d×1}=(D_{m×d}^{T}D_{m×d}+I_{d×d})^{-1}D_{m×d}^{T}C_{m×1}$</p><p>简化表示，采用A和b代表公式的主要部分。</p><p>$A=D_{m×d}^{T}D_{m×d}+I_{d×d}$</p><p>$b=D_{m×d}^{T}C_{m×1}$</p><p>则$θ_{d×1}=A^{-1}b$。</p><p>预测的期望收益为</p><p>$r=x_{d×1}^{T}θ_{d×1}$</p><p>置信区间上边界为</p><p>$b=α\sqrt{x_{d×1}^{T}(D_{m×d}^{T}D_{m×d}+I_{d×d})^{-1}x_{d×1}}$</p><p>$=α\sqrt{x_{d×1}^{T}A^{-1}x_{d×1}}$</p><p>臂的收益 = 预估期望收益 + 置信区间上界</p><p>按所有臂的收益进行排序，可以得到一个物品推荐列表。</p><h5 id="根据反馈结果更新参数"><a href="#根据反馈结果更新参数" class="headerlink" title="根据反馈结果更新参数"></a>根据反馈结果更新参数</h5><p>当一个臂被推荐出去之后，样本$D_{m×d}$变为$D_{(m+1)×d}$，A变为$A+x_{d×1}x_{d×1}^{T}$，b变为$b+rx_{d×1}$（r是回报），根据A和b可以更新θ。</p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>相比机器学习中的监督学习问题，基于环境的Bandit算法的评估更加困难。我们的目标是衡量bandit算法π的效果，每次臂的选择是基于前一次交互信息。<br>似乎唯一可行的评估方法就是在实时数据中运行该算法。我们能够收集到的只有之前采用不同策略获取到的离线日志，尚不清楚如何评估此类日志数据。这类评估问题被视为”离线策略评估问题“中的特例问题。</p><p>一种解决方案是建立一个在日志数据中模拟bandit过程的模拟器。然后，使用模拟器评估π。但是建立模拟器的过程会引入偏差，很难证明该方法的有效性。我们提出一种简单的方法实施，基于记录的数据且无偏差。</p><p>我们的目标是使用离线数据来评估bandit算法π。π是一个策略，根据历史$h_{t-1}$和上下文向量$x_{t1},…,x_{tK}$在时间t选择臂$a_{t}$。</p><p>采用算法<em>3</em>所示的评估方法。T个离线日志，一步一步遍历事件。在给定当前历史行为$h_{t-1}$的情况下，如果策略π选择一个与日志策略相同的臂a，则保留该事件，并将其添加到历史记录中，更新总收益$R_{T}$。否则当策略π选择一个与日志策略不同的臂a，则该算法忽略该日志，不进行任何操作。</p><p><img src="/2021/04/05/shen-du-xue-xi-tui-jian-xi-tong-shi-zhan-zhi-contextual-bandit/3.png" alt="3"></p><h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><p>雅虎首页的F1位置使用了该算法，并进行了为期一周的测试。雅虎的今日头条是Internet上访问量最大的模块。今日头条模块放置四篇高质量的文章，四篇文章是从人工编辑的文章库中选出，每小时刷新一次。如下图所示，默认F1处的文章大图显示。我们希望排名根据个人兴趣提供文章，并突出显示对于该访客最具吸引力的文章。</p><p><img src="/2021/04/05/shen-du-xue-xi-tui-jian-xi-tong-shi-zhan-zhi-contextual-bandit/2.png" alt="2"></p><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><h5 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h5><p>5月1日，以当天的日志（470万）为模型确定每种bandit算法的最佳参数，随后使用调整后的参数运行算法。</p><p>5月3日-9日，以为期一周的日志（3600万）来评估模型。</p><h5 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h5><p>用户特征</p><ul><li>人口统计信息：性别(2类)和年龄(离散为10段)</li><li>地理特征：全球和美国各州(200)</li><li>行为特征：1000个one-hot编码雅虎用户消费历史记录。</li></ul><p>文章特征</p><ul><li><p>URL类别：文章分类(数十)</p></li><li><p>人工编辑类别：人工编辑主题(数十)</p></li></ul><p>文章特征进行了one-hot编码，然后每个特征向量归一化为单位长度，增加了每个特征向量具有值为1的恒定特征。（为何增加一个特征向量为1的恒定特征呢？）</p><h5 id="结果对比"><a href="#结果对比" class="headerlink" title="结果对比"></a>结果对比</h5><p>算法分为三类：</p><p>I 未使用feature的bandit算法</p><ul><li><p>random</p></li><li><p>e-greedy</p></li><li><p>无所不知 策略达到最佳经验时，得出的无关上下文的点击率。首先计算每篇文章的经验点击率，然后始终选择最高经验点击率的文章。</p></li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>《A Contextual-Bandit Approach to<br>Personalized News Article Recommendation》<a href="https://arxiv.org/pdf/1003.0146.pdf">https://arxiv.org/pdf/1003.0146.pdf</a></p><p>《LinUCB论文的思想解读、场景应用与痛点说明》<a href="https://zhuanlan.zhihu.com/p/127189465">https://zhuanlan.zhihu.com/p/127189465</a></p><p>《Contextual Bandit算法在推荐系统中的实现及应用》<a href="https://zhuanlan.zhihu.com/p/35753281">https://zhuanlan.zhihu.com/p/35753281</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习推荐系统实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>未来几年，普通人可选择的三种投资方式</title>
      <link href="2021/04/04/wei-lai-de-san-chong-tou-zi-fang-shi/"/>
      <url>2021/04/04/wei-lai-de-san-chong-tou-zi-fang-shi/</url>
      
        <content type="html"><![CDATA[<p>未来几年，普通人可以选择的三种投资方式。</p><h5 id="一）一二线城市核心地段的房子。"><a href="#一）一二线城市核心地段的房子。" class="headerlink" title="一）一二线城市核心地段的房子。"></a>一）一二线城市核心地段的房子。</h5><p>这个不用说，未来几年北上广深的核心地段的房子一直都是保值的。</p><h5 id="二）龙头公司的优质股票。"><a href="#二）龙头公司的优质股票。" class="headerlink" title="二）龙头公司的优质股票。"></a>二）龙头公司的优质股票。</h5><p>随着大幅印钞，未来物价会跌，但是一些龙头公司的优质股票是一直保持升值的。现今这些公司一直处在高估值、高波动的状态。我们需要长期看好并且与泡沫共舞，从长远来看也可以获取不菲的收益。诚然，这种选择需要有一定的信心和技术门槛的。</p><h5 id="三）个人能力的投资。"><a href="#三）个人能力的投资。" class="headerlink" title="三）个人能力的投资。"></a>三）个人能力的投资。</h5><p>自身和子女教育是最好的投资方式。几乎零成本而且终身收益。这跟钱多少没有关系，现在社会学习渠道和内容很多，个人提升不一定非要上清华北大。罗永浩中学文聘，曹德旺、李嘉诚小学文聘，鲁迅中学文聘。但是，中学文聘不等于中学水平。除了学习之外，创业试错，出国游学，大企业就职，参观展会，职业教育，读书演讲等这些都是提高个人能力。可能有人反驳，不是学习就一定有回报吗？不是7万研究生去送外卖了吗？而且很多人没怎么读书却成了社会精英，而有些人读了几十年的书也没有发财呢？这就需要知道读书和学习的关系。</p><ul><li>读书不是学习的唯一方式。</li><li>文聘不等于认知水平。</li><li>要承认每个人有不同的天赋。</li></ul><p>读书这件事不应该做横向比较，更应该和自身做对比。读书不一定是成功的全部因素，但也一定是必要因素。一个知识量大的人也许赚不到多少钱，但也许没有这些知识，他可能混的更惨。所以把多余的钱砸在自己和孩子个人能力提高上是一件性价比高且很有必要的一件事。</p>]]></content>
      
      
      <categories>
          
          <category> 人生感悟 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人生感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习推荐系统实战之Bandit</title>
      <link href="2021/04/02/shen-du-xue-xi-tui-jian-xi-tong-shi-zhan-zhi-bandit/"/>
      <url>2021/04/02/shen-du-xue-xi-tui-jian-xi-tong-shi-zhan-zhi-bandit/</url>
      
        <content type="html"><![CDATA[<h2 id="Bandit起源"><a href="#Bandit起源" class="headerlink" title="Bandit起源"></a>Bandit起源</h2><p>Bandit算法来源于历史悠久的赌博学。想象这样一个场景：一个赌徒走进赌场，一眼看去是一排外表一摸一样的老虎机。每个老虎机吐钱的概率不一样，他应该在每次如何选择老虎机才可以做到最大化收益？这就是多臂赌博机问题（Muti-arm Bandits，简称MAB）。一系列应对该问题的策略统称为Bandit算法。</p><p><img src="/2021/04/02/shen-du-xue-xi-tui-jian-xi-tong-shi-zhan-zhi-bandit/1.png" alt="1"></p><p>现实生活中我们也会遇到一系列类似的场景：</p><ol><li>新用户进入系统，如何得知他对哪个类别更感兴趣？</li><li>有若干广告库物料，如何展示广告，才能获得最大收益？是一直展示收益最高的吗？</li><li>玩家进入新游戏，哪个关卡是他最感兴趣的，如何合理排序？</li></ol><p>解决这类问题最好的方法就是去试，但也不是随便试，而是有策略的试，通过多次实验摸透每个老虎机背后的赢钱规律（概率分布）。</p><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><h5 id="累计遗憾"><a href="#累计遗憾" class="headerlink" title="累计遗憾"></a>累计遗憾</h5><p>Bandit算法的思想是，看看每次选择会带来多少遗憾，累计遗憾越少越好。</p><p>$R_{T}=\sum_{i=1}^{T}(w_{opt}-w_{B(i)})=Tw^{*} - \sum_{i=1}^{T}w_{B(i)}$</p><p>$w_{opt}$是每次最优选择得到的收益，$w_{B(i)}$是每次实际选择得到的收益，二者的差就是遗憾的量化。进行T次选择之后，就获得累计遗憾$R_{T}$。</p><h2 id="Bandit算法"><a href="#Bandit算法" class="headerlink" title="Bandit算法"></a>Bandit算法</h2><p>为了更好的学习Bandit算法，我们可以带入下场景。一名拉斯维加斯赌徒走进了赌城，面前有5台老虎机，每台老虎机上都有一个拉杆，如何通过有限的尝试知道哪台老虎机赢钱概率最大？下面介绍多种应对的策略。</p><h5 id="朴素bandit"><a href="#朴素bandit" class="headerlink" title="朴素bandit"></a>朴素bandit</h5><p>随机试若干次，计算每个臂的平均收益，一直选均值最大的那个臂。这个算法是在实际生活中最常采用的。</p><h5 id="Epsilon-Greedy"><a href="#Epsilon-Greedy" class="headerlink" title="Epsilon-Greedy"></a>Epsilon-Greedy</h5><p>Epsilon贪婪算法原理</p><ol><li>在(0, 1)之间选择一个数，名为epsilon。</li><li>每次以epsilon概率随机选择一个臂，以1-epsilon概率选择平均收益最大的臂。</li></ol><h5 id="UCB"><a href="#UCB" class="headerlink" title="UCB"></a>UCB</h5><p>置信区间上界（Upper Confidence Bound，UCB），为每个臂评分，选择评分最高的候选臂。然后观察用户反馈，更新候选臂。</p><p>$x_{j}^{-}(t) + \sqrt{\frac{2lnt}{T_{j,t}}}$</p><p>t是目前的总选择次数，$x_{j}^{-}$是每个臂的平均收益，$T_{j,t}$是j臂的选择次数。由公式可知，一个候选臂被选择次数$T_{j,t}$越小，它的价值越大，排序越靠前。（从公式上可以看出，除了臂本身的平均收益之外，增加了一个选择次数的纬度，相同次数下，选择次数越少，值越大。）</p><h5 id="Thompson-Sample"><a href="#Thompson-Sample" class="headerlink" title="Thompson Sample"></a>Thompson Sample</h5><p>汤普森采样算法原理</p><ol><li>假设每个臂背后的概率分布是Beta分布。</li><li>每次选择，每个臂都会出一个随机数，然后按随机数排序，输出最大随机数的臂对应的物品。</li></ol><p>代入到推荐场景中。Beta分布有两个参数，a参数代表用户点击次数，b参数代表用户没有点击次数，然后进行汤普森采样。</p><ol><li>取出每一个候选臂对应的参数a和b。</li><li>每个候选臂根据Beta(a,b)产生一个随机数。</li><li>按随机数排序，输出最大值对应的候选臂。</li><li>根据用户反馈，用户点击则a+1，否则b+1。</li></ol><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> math<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">class</span> <span class="token class-name">Bandits</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>n_bandits <span class="token operator">=</span> <span class="token number">10</span> <span class="token comment" spellcheck="true"># 10组老虎机</span>        self<span class="token punctuation">.</span>n_arms <span class="token operator">=</span> <span class="token number">5</span> <span class="token comment" spellcheck="true"># 5台老虎机（每台老虎机1个臂）</span>        self<span class="token punctuation">.</span>n_pulls <span class="token operator">=</span> <span class="token number">1000</span> <span class="token comment" spellcheck="true"># 总共拉1000次</span>        self<span class="token punctuation">.</span>strategies <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'random'</span><span class="token punctuation">,</span> <span class="token string">'epsilon_greedy'</span><span class="token punctuation">,</span> <span class="token string">'ucb'</span><span class="token punctuation">,</span> <span class="token string">'thompson_sample'</span><span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># epsilon_greedy参数</span>        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> <span class="token number">0.3</span>        self<span class="token punctuation">.</span>decay_rate <span class="token operator">=</span> <span class="token number">0.999</span>        self<span class="token punctuation">.</span>min_epsilon <span class="token operator">=</span> <span class="token number">0.1</span>            <span class="token keyword">def</span> <span class="token function">pick_arm</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> strategy<span class="token punctuation">,</span> q_values<span class="token punctuation">,</span> counts<span class="token punctuation">,</span> success<span class="token punctuation">,</span> failure<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">""" 选择臂        :strategy 策略        :q_value 每个臂的收益        :counts 总次数        :success 收益        :failure 遗憾        """</span>        <span class="token keyword">if</span> strategy <span class="token operator">==</span> <span class="token string">'random'</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>q_values<span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">elif</span> strategy <span class="token operator">==</span> <span class="token string">'epsilon_greedy'</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># [0.1, 0.3]</span>            epsilon <span class="token operator">=</span> max<span class="token punctuation">(</span>self<span class="token punctuation">.</span>epsilon<span class="token operator">*</span>self<span class="token punctuation">.</span>decay_rate<span class="token punctuation">,</span> self<span class="token punctuation">.</span>min_epsilon<span class="token punctuation">)</span>            <span class="token keyword">if</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> epsilon<span class="token punctuation">:</span>                best_arms_value <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>q_values<span class="token punctuation">)</span>                best_arms <span class="token operator">=</span> np<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>q_values <span class="token operator">==</span> best_arms_value<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token keyword">return</span> best_arms<span class="token punctuation">[</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>best_arms<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                <span class="token keyword">return</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>q_values<span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token keyword">elif</span> strategy <span class="token operator">==</span> <span class="token string">'ucb'</span><span class="token punctuation">:</span>            total_counts <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>counts<span class="token punctuation">)</span>            ucb_q_values <span class="token operator">=</span> q_values <span class="token operator">+</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>np<span class="token punctuation">.</span>reciprocal<span class="token punctuation">(</span>counts<span class="token operator">+</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">2</span><span class="token operator">*</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>total_counts<span class="token operator">+</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            best_arms_value <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>ucb_q_values<span class="token punctuation">)</span>            best_arms <span class="token operator">=</span> np<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>ucb_q_values <span class="token operator">==</span> best_arms_value<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> best_arms<span class="token punctuation">[</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>best_arms<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>                <span class="token keyword">elif</span> strategy <span class="token operator">==</span> <span class="token string">'thompson_sample'</span><span class="token punctuation">:</span>            <span class="token triple-quoted-string string">""" 利用beta分布选择杆 """</span>            sample_means <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>len<span class="token punctuation">(</span>counts<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>counts<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                sample_means<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>beta<span class="token punctuation">(</span>success<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> failure<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>sample_means<span class="token punctuation">)</span>                <span class="token keyword">def</span> <span class="token function">plot_result</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">""" 绘制曲线 """</span>        fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>        ax <span class="token operator">=</span> fig<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span>                <span class="token keyword">for</span> st <span class="token keyword">in</span> self<span class="token punctuation">.</span>strategies<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 初始化</span>            <span class="token comment" spellcheck="true"># 记录10x5台老虎机拉杆次数</span>            best_arm_counts <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_bandits<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_pulls<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_bandits<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># 每个杆是最大收益的概率</span>                arm_prob <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_arms<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># 收益最大的杆</span>                best_arm <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>arm_prob<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># 收益率</span>                q_values <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_arms<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># 统计拉杆次数</span>                counts <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_arms<span class="token punctuation">)</span>                success <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_arms<span class="token punctuation">)</span>                failure <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_arms<span class="token punctuation">)</span>                                <span class="token comment" spellcheck="true"># 拉1000次杆</span>                <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_pulls<span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token comment" spellcheck="true"># 根据不同策略选择臂</span>                    _arm <span class="token operator">=</span> self<span class="token punctuation">.</span>pick_arm<span class="token punctuation">(</span>st<span class="token punctuation">,</span> q_values<span class="token punctuation">,</span> counts<span class="token punctuation">,</span> success<span class="token punctuation">,</span> failure<span class="token punctuation">)</span>                    <span class="token comment" spellcheck="true"># 选择臂的收益</span>                    reward <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>binomial<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> arm_prob<span class="token punctuation">[</span>_arm<span class="token punctuation">]</span><span class="token punctuation">)</span>                    <span class="token comment" spellcheck="true"># 拉杆次数统计</span>                    counts<span class="token punctuation">[</span>_arm<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>                    <span class="token comment" spellcheck="true"># 更新收益</span>                    q_values<span class="token punctuation">[</span>_arm<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token punctuation">(</span>reward<span class="token operator">-</span>q_values<span class="token punctuation">[</span>_arm<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> counts<span class="token punctuation">[</span>_arm<span class="token punctuation">]</span>                    <span class="token comment" spellcheck="true"># 成功的收益</span>                    success<span class="token punctuation">[</span>_arm<span class="token punctuation">]</span> <span class="token operator">+=</span> reward                    <span class="token comment" spellcheck="true"># 失败的收益</span>                    failure<span class="token punctuation">[</span>_arm<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>reward<span class="token punctuation">)</span>                    best_arm_counts<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> counts<span class="token punctuation">[</span>best_arm<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true">#print('====== st:{}, i:{} '.format(st, i))</span>                <span class="token comment" spellcheck="true">#print('arm_prob: ', arm_prob)</span>                            <span class="token comment" spellcheck="true"># 横纵坐标</span>            y <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>best_arm_counts<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            x <span class="token operator">=</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true">#print('strategy: ', st)</span>            ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> label<span class="token operator">=</span>st<span class="token punctuation">)</span>            ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>                plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'steps'</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'optimal pull'</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">110</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>bandits <span class="token operator">=</span> Bandits<span class="token punctuation">(</span><span class="token punctuation">)</span>bandits<span class="token punctuation">.</span>plot_result<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><img src="/2021/04/02/shen-du-xue-xi-tui-jian-xi-tong-shi-zhan-zhi-bandit/2.png" alt="2"></p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>random表现最差，采用5个里面随机选择一个杆的方式，选择最高收益杆的概率趋近于0.2。</p><p>汤普森采样表现最好，能够最快速的找到收益最大的杆，其次是Epsilon-Greedy、UCB。</p>]]></content>
      
      
      <categories>
          
          <category> 推荐算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习推荐系统实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之Q-learning</title>
      <link href="2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/"/>
      <url>2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/</url>
      
        <content type="html"><![CDATA[<p>Q-Learning，是另一种TD算法，DQN就是Q-learning的一种实现方式。</p><h2 id="Sarsa-vs-Q-Learning"><a href="#Sarsa-vs-Q-Learning" class="headerlink" title="Sarsa vs Q-Learning"></a>Sarsa vs Q-Learning</h2><p>sarsa和Qlearning都是TD算法，但解决的问题不同。</p><h5 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h5><p>Sarsa是训练价值动作函数，$Q_{π}(s,a)$</p><p>TD target: $y_{t} = r_{t} + γ·Q_{π}(s_{t+1}, a_{t+1})$</p><p>我们使用sarsa更新价值网络(critic)。</p><h5 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h5><p>Q-learning是训练最优动作价值函数$Q^{*}(s,a)$</p><p>TD target: $y_{t} = r_{t} + γ·max_{a}Q^{*}(s_{t+1}, a)$</p><p>我们使用Q-learning更新DQN。</p><h2 id="推导TD-target"><a href="#推导TD-target" class="headerlink" title="推导TD target"></a>推导TD target</h2><p>$Q_{π}(s_{t}, a_{t})=E[R_{t}+γ·Q_{π}(S_{t+1}, A_{t+1})]$</p><p>如果π是最优策略$π^{*}$</p><p>$Q_{π*}(s_{t}, a_{t})=E[R_{t}+γ·Q_{π^{*}}(S_{t+1}, A_{t+1})]$</p><p>$Q_{π*}$和$Q^{*}$都表示最优动作价值函数。</p><p>$Q*(s_{t},a_{t})=E[R_{t}+γ·Q*(S_{t+1},A_{t+1})]$</p><p>动作$A_{t+1}$可以计算为</p><p>$A_{t+1}=argmax_{a}Q*(S_{t+1},a)$</p><p>所以，$Q*(s_{t},a_{t})=E[R_{t}+γ·max_{a}Q*(S_{t+1},a)]$</p><p>直接求期望困难，所以做蒙特卡洛近似，期望中有$R_{t}$用$r_{t}$做近似，$S_{t+1}$用$s_{t}$近似。</p><p>$Q*(s_{t},a_{t})=E[R_{t}+γ·max_{a}Q*(S_{t+1},a)]≈r_{t}+γ·max_{a}Q*(s_{t+1},a)$ (TD target)</p><h2 id="Q-Learning的表格形式"><a href="#Q-Learning的表格形式" class="headerlink" title="Q-Learning的表格形式"></a>Q-Learning的表格形式</h2><h5 id="Q-Learning的表格形式-1"><a href="#Q-Learning的表格形式-1" class="headerlink" title="Q-Learning的表格形式"></a>Q-Learning的表格形式</h5><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/1.png" alt="1"></p><h5 id="学习流程"><a href="#学习流程" class="headerlink" title="学习流程"></a>学习流程</h5><ol><li>观测四元组$(s_{t}, a_{t}, r_{t}, s_{t+1})$</li><li>TD target: $y_{t} = r_{t} + γ·max_{a}Q^{*}(s_{t+1}, a)$</li><li>TD error: $σ_{t}=Q^{*}(s_{t}, a_{t})-y_{t}$</li><li>更新参数: $Q*(s_{t}, a_{t})&lt;-Q*(s_{t}, a_{t})-α·σ_{t}$</li></ol><h2 id="Q-Learning价值网络形式"><a href="#Q-Learning价值网络形式" class="headerlink" title="Q-Learning价值网络形式"></a>Q-Learning价值网络形式</h2><h5 id="Q-Learning价值网络形式-1"><a href="#Q-Learning价值网络形式-1" class="headerlink" title="Q-Learning价值网络形式"></a>Q-Learning价值网络形式</h5><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/2.png" alt="2"></p><h2 id="Q-Learning总结"><a href="#Q-Learning总结" class="headerlink" title="Q-Learning总结"></a>Q-Learning总结</h2><p>目标：学习最优动作价值函数$Q^{*}$<br>形式：</p><ul><li><p>表格</p><ul><li>直接学习$Q*$</li><li>有限的状态和动作</li><li>绘制表格，通过Q-learning更新表格的动作价值</li></ul></li><li><p>价值神经网络</p><ul><li>根据DQN$Q(s,a;w)$近似$Q*$</li><li>使用Q-learning更新参数w</li></ul></li></ul><h2 id="Q-Learning代码实现"><a href="#Q-Learning代码实现" class="headerlink" title="Q-Learning代码实现"></a>Q-Learning代码实现</h2><h5 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h5><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/3.png" alt="3"></p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> time<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pdnp<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 产生一组固定的随机序列</span>N_STATES <span class="token operator">=</span> <span class="token number">6</span> <span class="token comment" spellcheck="true"># 状态点数量</span>ACTIONS <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'left'</span><span class="token punctuation">,</span> <span class="token string">'right'</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 动作</span>EPSILON <span class="token operator">=</span> <span class="token number">0.9</span> <span class="token comment" spellcheck="true"># greedy</span>ALPHA <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token comment" spellcheck="true"># learning rate</span>GAMMA <span class="token operator">=</span> <span class="token number">0.9</span> <span class="token comment" spellcheck="true"># 衰减因子</span>MAX_EPISODE <span class="token operator">=</span> <span class="token number">10</span> <span class="token comment" spellcheck="true"># 迭代次数</span>FRESH_TIME <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token comment" spellcheck="true"># 刷新时间</span><span class="token keyword">def</span> <span class="token function">build_qtable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">""" 建立一个s×a的Q表 """</span>    q_table <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>        np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>N_STATES<span class="token punctuation">,</span> len<span class="token punctuation">(</span>ACTIONS<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        columns<span class="token operator">=</span>ACTIONS    <span class="token punctuation">)</span>    <span class="token keyword">return</span> q_table<span class="token keyword">def</span> <span class="token function">choose_action</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> q_table<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">""" 90%的概率选当前状态下q值大的动作，10%的概率随机选动作 """</span>    state_actions <span class="token operator">=</span> q_table<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>state<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> EPSILON <span class="token operator">or</span> state_actions<span class="token punctuation">.</span>all<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        action_name <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>ACTIONS<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        action_name <span class="token operator">=</span> ACTIONS<span class="token punctuation">[</span>state_actions<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> action_name<span class="token keyword">def</span> <span class="token function">get_env_feedback</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">""" 选定动作，从环境中得到奖励r和下一次的状态_s """</span>    <span class="token keyword">if</span> a <span class="token operator">==</span> <span class="token string">'right'</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># agent到达目的地，获得奖励</span>        <span class="token keyword">if</span> s <span class="token operator">==</span> N_STATES <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">:</span>            _s <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>            r <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            _s <span class="token operator">=</span> s<span class="token operator">+</span><span class="token number">1</span>            r <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        r <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">if</span> s <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            _s <span class="token operator">=</span> s        <span class="token keyword">else</span><span class="token punctuation">:</span>            _s <span class="token operator">=</span> s<span class="token number">-1</span>    <span class="token keyword">return</span> _s<span class="token punctuation">,</span> r    <span class="token keyword">def</span> <span class="token function">update_env</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> episode<span class="token punctuation">,</span> step_counter<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">""" 更新环境 """</span>    env_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'-'</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token punctuation">(</span>N_STATES<span class="token number">-1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'T'</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># '------T'</span>    <span class="token keyword">if</span> s <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'episode: {}, step_counter: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>episode<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> step_counter<span class="token punctuation">)</span><span class="token punctuation">)</span>        time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        env_list<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'o'</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>env_list<span class="token punctuation">)</span><span class="token punctuation">)</span>        time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span>FRESH_TIME<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">rl</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 1. 初始化一张s×a的q表，初始值均为0。</span>    q_table <span class="token operator">=</span> build_qtable<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'init q-table:\n'</span><span class="token punctuation">,</span> q_table<span class="token punctuation">)</span>        <span class="token keyword">for</span> episode <span class="token keyword">in</span> range<span class="token punctuation">(</span>MAX_EPISODE<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 2. agent和环境初始化，agent的状态s位于出发点。</span>        step_counter <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment" spellcheck="true"># q表迭代次数</span>        s <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment" spellcheck="true"># 初始位置</span>        is_terminated <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token comment" spellcheck="true"># 回合结束（到达终点）</span>        update_env<span class="token punctuation">(</span>s<span class="token punctuation">,</span> episode<span class="token punctuation">,</span> step_counter<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 初始化agent和环境</span>                <span class="token comment" spellcheck="true"># 3. 根据实际-预估的误差来更新q值，q值（动作价值）越来越准。</span>        <span class="token keyword">while</span> <span class="token operator">not</span> is_terminated<span class="token punctuation">:</span>            a <span class="token operator">=</span> choose_action<span class="token punctuation">(</span>s<span class="token punctuation">,</span> q_table<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 选一个行为</span>            _s<span class="token punctuation">,</span> r <span class="token operator">=</span> get_env_feedback<span class="token punctuation">(</span>s<span class="token punctuation">,</span> a<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 获取下一个状态和收益</span>            <span class="token comment" spellcheck="true">#print('s, a: ', s, a)</span>            q_predict <span class="token operator">=</span> q_table<span class="token punctuation">.</span>loc<span class="token punctuation">[</span>s<span class="token punctuation">,</span> a<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 获取预估的q值</span>            <span class="token keyword">if</span> _s <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>                q_target <span class="token operator">=</span> r <span class="token operator">+</span> GAMMA<span class="token operator">*</span>q_table<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>_s<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                q_target <span class="token operator">=</span> r                is_terminated <span class="token operator">=</span> <span class="token boolean">True</span>                        q_table<span class="token punctuation">.</span>loc<span class="token punctuation">[</span>s<span class="token punctuation">,</span> a<span class="token punctuation">]</span> <span class="token operator">+=</span> ALPHA <span class="token operator">*</span> <span class="token punctuation">(</span>q_target <span class="token operator">-</span> q_predict<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 实际-预估的误差，更新q值</span>            s <span class="token operator">=</span> _s                        update_env<span class="token punctuation">(</span>s<span class="token punctuation">,</span> episode<span class="token punctuation">,</span> step_counter<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 更新环境</span>            step_counter <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'===== q_table:\n'</span><span class="token punctuation">,</span> q_table<span class="token punctuation">)</span>    <span class="token keyword">return</span> q_table</code></pre><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/4.png" alt="4"><br>根据上图可以发现，20轮学习后，小车可以径直走向终点。q-table表中越靠近终点的state，action=’right’的动作价值(q值)越高。</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之sarsa</title>
      <link href="2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/"/>
      <url>2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/</url>
      
        <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Sarsa是TD算法的一种。算法每次用一个五元组$(s_{t},a_{t},r_{t},s_{t+1},a_{t+1})$来更新$Q_{π}$，全称State-action-reward-state-action，简称SARSA。</p><h2 id="TD-target推导"><a href="#TD-target推导" class="headerlink" title="TD target推导"></a>TD target推导</h2><p>当前时刻的回报$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/1.png" alt="1"><br>得出，$U_{t}=R_{t} + γ·U_{t+1}$。</p><p>其中，$R_{t}$依赖于$(S_{t}, A_{t}, S_{t+1})$</p><p>$Q_{π}(s_{t},a_{t})=E[U_{t}|s_{t},a_{t}]$</p><p>= $E[R_{t}+γ·U_{t+1}|s_{t},a_{t}]$</p><p>= $E[R_{t}|s_{t},a_{t}]+γ·E[U_{t+1}|s_{t},a_{t}]$</p><p>= $E[R_{t}|s_{t},a_{t}]+γ·E[Q_{π}(S_{t+1},A_{t+1})|s_{t},a_{t}]$<br><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/2.png" alt="2"></p><p>目标是使动作价值$Q_{π}$更接近$y_{t}$。</p><h2 id="Sarsa表格形式"><a href="#Sarsa表格形式" class="headerlink" title="Sarsa表格形式"></a>Sarsa表格形式</h2><h5 id="Sars的表格形式"><a href="#Sars的表格形式" class="headerlink" title="Sars的表格形式"></a>Sars的表格形式</h5><ul><li>我们想要学习$Q_{π}(s,a)$</li><li>状态和动作都是有限的。</li><li>表中每个元素对应一个动作价值，每次更新表格中的一个元素。</li></ul><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/3.png" alt="3"></p><h5 id="学习流程"><a href="#学习流程" class="headerlink" title="学习流程"></a>学习流程</h5><ol><li>每次观测一个四元组$(s_{t},a_{t},r_{t},s_{t+1})$</li><li>根据policy函数π和$s_{t+1}$随机抽样一个$a_{t+1}$</li><li>TD target: $y_{t}=r_{t}+γ·[Q_{π}(s_{t+1},a_{t+1})$</li><li>TD error: $σ_{t}=Q_{π}(s_{t},a_{t})-y_{t}$</li><li>更新动作价值Q: $σ_{t}(s_{t}, a_{t})&lt;-Q_{π}(s_{t},a_{t})-α·σ_{t}$</li></ol><h2 id="Sarsa价值网络形式"><a href="#Sarsa价值网络形式" class="headerlink" title="Sarsa价值网络形式"></a>Sarsa价值网络形式</h2><h5 id="Sarsa价值网络形式-1"><a href="#Sarsa价值网络形式-1" class="headerlink" title="Sarsa价值网络形式"></a>Sarsa价值网络形式</h5><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/4.png" alt="4"></p><ul><li>通过价值网络q(s,a;w)近似$Q_{π}(s,a)$</li><li>q作为一个评论家来评估演员的好坏。（Actor-Critic方法）</li><li>学习参数w。</li></ul><h5 id="学习流程-1"><a href="#学习流程-1" class="headerlink" title="学习流程"></a>学习流程</h5><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/5.png" alt="5"></p><h2 id="Sarsa总结"><a href="#Sarsa总结" class="headerlink" title="Sarsa总结"></a>Sarsa总结</h2><p>目标：学习动作价值函数$Q_{π}$<br>形式：</p><ul><li><p>表格</p><ul><li>直接学习Qπ</li><li>有限的状态和动作</li><li>绘制表格，通过sarsa更新表格的动作价值</li></ul></li><li><p>价值神经网络</p><ul><li>根据价值网络$q(s,a;w)$近似$Q_{π}$</li><li>使用sarsa更新参数</li><li>应用：actor-critic方法</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之Actor-Critic</title>
      <link href="2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/"/>
      <url>2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/</url>
      
        <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Actor-Critic合并了策略学习和价值学习两种强化学习算法。Actor是策略网络，用来控制agent运动，可以看做演员；Critic是价值网络，用来给动作打分，可以看做评论家员。</p><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/1.png" alt="1"></p><h5 id="价值网络和策略网络"><a href="#价值网络和策略网络" class="headerlink" title="价值网络和策略网络"></a>价值网络和策略网络</h5><p>状态价值函数</p><p>$V_{π}=\sum_{a}π(a|s)·Q_{π}(s,a)≈\sum_{a}π(a|s;θ)·Q_{π}(s,a;w)$</p><p>公式中π和$Q_{π}$都是未知的。</p><p>策略网络(actor)</p><ul><li>使用神经网络$π(a|s;θ)$去近似$π(a|s)$</li><li>神经网络训练参数θ</li></ul><p>价值网络(critic)</p><ul><li>使用神经网络$q(s,a;w)$去近似$Q_{π}(s,a)$</li><li>神经网络训练参数w</li></ul><p>actor是个演员可以做动作，但他不知道什么是好的动作，这时候就需要评论家critic来评判。学习这两个网络的目的是让演员的平均分越来越高，同时让评论家的打分越来越准。</p><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/2.png" alt="2"></p><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/3.png" alt="3"></p><h5 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h5><p>使用神经网络近似这两个函数</p><p>$V(s;θ,w)=\sum_{a}π(a|s;θ)·q(s,a;w)$</p><p>更新参数θ和w</p><p>更新策略网络$π(a|s;θ)$为了增加状态值$V(s;θ,w)$。</p><p>更新价值网络q(s,a;w)为了打分越来越精准。</p><ol><li>观测状态$s_{t}$。</li><li>随机抽样根据$π(·|s_{t};θ_{t})$得到动作$a_{t}$。</li><li>完成$a_{t}$并观测新的状态$s_{t+1}$和奖励$r_{t}$。</li><li>更新w(价值网络参数)使用时间差异(TD)。</li><li>更新θ(策略网络参数)使用策略梯度。</li></ol><h5 id="使用TD算法更新价值网络q"><a href="#使用TD算法更新价值网络q" class="headerlink" title="使用TD算法更新价值网络q"></a>使用TD算法更新价值网络q</h5><ul><li>计算$q(s_{t},a_{t};w_{t})$和$q(s_{t+1},a_{t+1};w_{t})$</li><li>TD target: $y_{t}=r_{t}+γ·q(s_{t+1},a_{t+1};w_{t})$</li><li>损失: $L(w)=\frac{1}{2}[q(s_{t},a_{t};w)-y_{t}]^{2}$</li><li>梯度下降: $w_{t+1}=w_{t}-α·\frac{∂L(w)}{∂w}|w=w_{t}$</li></ul><p>使得评论家打分更准。</p><h5 id="使用策略梯度更新策略网络π"><a href="#使用策略梯度更新策略网络π" class="headerlink" title="使用策略梯度更新策略网络π"></a>使用策略梯度更新策略网络π</h5><ul><li><p>神经网络近似状态价值函数$V(s;θ,w)=\sum_{a}π(a|s;θ)·q(s,a;w)$</p></li><li><p>策略梯度，更新$V(s_{t};θ,w)$的θ</p></li></ul><p>$g(a,θ)=\frac{∂logπ(a|s,θ)}{∂θ}·q(s_{t},a;w)$</p><p>$\frac{∂V(s;θ,w_{t})}{∂θ}=E_{A}[g(A, θ)]$</p><p>根据π函数随机采样a，$θ_{t+1}=θ_{t}+β·g(a,θ_{t})$。</p><p>使得演员得分更高。</p><h5 id="Actor-Critic总结"><a href="#Actor-Critic总结" class="headerlink" title="Actor-Critic总结"></a>Actor-Critic总结</h5><ol><li>观测状态点$s_{t}$和随机抽样$a_{t}$~$π(·|s_{t};θ_{t})$</li><li>完成$a_{t}$，环境给一个新的状态点$s_{t+1}$和回报$r_{t}$</li><li>随机抽样$a_{t+1}^{-}$~$π(·|s_{t};θ_{t})$，注意并未执行$a_{t+1}^{-}$</li><li>评估价值网络$q_{t}=q(s_{t},a_{t};w_{t})$和$q_{t+1}=q(s_{t+1},a_{t+1}^{-};w_{t})$</li><li>计算TD error: $σ_{t}=q_{t}-(r_{t}+γ·q_{t+1})$</li><li>价值网络求导: $d_{w,t}=\frac{∂q(s_{t},a_{t};w)}{∂w}|w=w_{t}$</li><li>更新价值网络: $w_{t+1}=w_{t}-α·σ_{t}·d_{w,t}$</li><li>策略网络求导: $d_{θ,t}=\frac{∂logπ(a_{t}|s_{t};θ)}{∂θ}|θ=θ_{t}$</li><li>更新策略网络: $θ_{t+1}=θ_{t}+β·q_{t}·d_{θ,t}$</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之策略学习</title>
      <link href="2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/"/>
      <url>2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="策略学习"><a href="#策略学习" class="headerlink" title="策略学习"></a>策略学习</h2><h5 id="policy函数"><a href="#policy函数" class="headerlink" title="policy函数"></a>policy函数</h5><p>策略，根据观测的状态来做出决策来控制agent的运动。</p><p>policy函数，$π(s, a)$，范围[0,1]，其中</p><p>$π(s, a) = P(A=a|S=s)$，根据policy函数，可以求出给定状态s，采取动作a的概率。</p><p>理想中，我们可以采用$s × a$的表代表policy的概率，但对于超级玛丽这种游戏，有无数种状态，需要用一个policy网络来近似。</p><h5 id="policy-network"><a href="#policy-network" class="headerlink" title="policy network"></a>policy network</h5><p>策略网络（policy network）$π(a|s;θ)$近似$π(a|s)$。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/1.png" alt="1"></p><p>$\sum_{a∈A}(a|s;θ)=1$，其中A={‘left’, ‘right’, ‘up’}，是一个动作集合。</p><h5 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h5><p>return，当前和未来的累计奖励</p><p>$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><h5 id="动作值函数"><a href="#动作值函数" class="headerlink" title="动作值函数"></a>动作值函数</h5><p>$Q_π(S_{t}, a_{t})=E[U_{t}|S_{t}=s_{t},A_{t}=a_{t}]$</p><h5 id="状态值函数"><a href="#状态值函数" class="headerlink" title="状态值函数"></a>状态值函数</h5><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\sum_{a}π(a|s_{t})·Q_{π}(s_{t}, a)$</p><p><strong>近似状态值函数</strong></p><p>$V_{π}(s_{t};θ)=E_{A}[Q_{π}(s_{t}, A)]=\sum_{a}π(a|s_{t};θ)·Q_{π}(s_{t}, a)$</p><p>目标是最大化状态价值的期望，$J(θ)=E_{S}[V(S;θ)]$</p><p>如何更新θ呢？</p><p>$θ &lt;- θ + β·\frac{∂V(s;θ)}{∂θ}$</p><h5 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h5><ul><li>第一种形式</li></ul><p>$\frac{∂V(s;θ)}{∂θ}=\frac{∂\sum_{a}π(a|s;θ)·Q_{π}(s,a)}{∂θ}$</p><p>= $\sum_{a}\frac{∂π(a|s;θ)·Q_{π}(s,a)}{∂θ}$</p><p>= $\sum_{a}\frac{∂π(a|s;θ)}{∂θ}·Q_{π}(s,a)$ // 假设θ是独立于$Q_{π}$。</p><ul><li>第二种形式</li></ul><p>$\frac{∂V(s;θ)}{∂θ}$ = $\sum_{a}\frac{∂π(a|s;θ)}{∂θ}·Q_{π}(s,a)$</p><p>= $\sum_{a}π(a|s;θ)·\frac{∂logπ(a|s;θ)}{∂θ}$</p><p>这两种形式是等价的，离散的使用第一种，连续的使用第二种。</p><p>对于离散的动作，我们取第一种。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/2.png" alt="2"></p><p>对于连续的动作，可以有[0,1]所有的实数，动作有无数种，无法做动作的加和，故使用第二个公式。<br>先通过policyπ函数随机抽样一个a，然后根据蒙特卡洛近似出g，使用g近似策略梯度。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/3.png" alt="3"></p><h5 id="策略梯度算法总结"><a href="#策略梯度算法总结" class="headerlink" title="策略梯度算法总结"></a>策略梯度算法总结</h5><ol><li>观测到$s_t$</li><li>策略网络随机抽样动作$a_t$</li><li>计算t时刻的动作价值$q_{t}≈Q_{π}(s_{t}, a_{t})$</li><li>对策略网络求导$d_{θ,t}=\frac{∂logπ(a_{t}|s_{t};θ)}{∂θ}|θ=θ_{t}$</li><li>近似策略梯度$g(a_{t},θ_{t})=q_{t}·d_{θ,t}$</li><li>更新策略网络$θ_{t+1}=θ_{t}+β·g(a_{t},θ_{t})$</li></ol><h5 id="如何计算-Q-π-s-t-a-t"><a href="#如何计算-Q-π-s-t-a-t" class="headerlink" title="如何计算$Q_{π}(s_{t},a_{t})$"></a>如何计算$Q_{π}(s_{t},a_{t})$</h5><p>方式一：增强(reinforce)</p><p>用策略网络π控制小球运动，从开始玩到游戏结束。s1,a1,r1,s2,a2,r2…sT,aT,rT，然后计算所有奖励r的加权求和。</p><p>计算所有r的$u_{t}=\sum_{k=t}^{T}r^{k-t}r_{k}$</p><p>因为$Qπ(s_{t},a_{t})=E[U_{t}]$，我们使用$u_{t}$近似$Qπ(s_{t},a_{t})$。</p><p>方式二：使用神经网络近似$Q_{π}$</p><p>这样就有了两个网络，一个用来近似π，一个用来近似$Q_{π}$</p><h2 id="策略学习总结"><a href="#策略学习总结" class="headerlink" title="策略学习总结"></a>策略学习总结</h2><ol><li>我们希望得到策略函数π，然后用π自动控制agent运动。每当agent观察st，agent就用π自动算出at。</li><li>直接求策略函数比较困难，故用神经网络近似策略函数，这个网络称为策略网络(policy network)$π(a|s;θ)$。</li><li>根据策略梯度学习策略网络。</li><li>策略梯度是状态价值函数V关于θ的导数$E_{S}[V(S;θ)]$。（注：目标函数是状态价值V关于状态的期望）</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之价值学习</title>
      <link href="2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/"/>
      <url>2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><h5 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h5><p>回报，未来的累计奖励。</p><p>$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><p>回报取决于动作$A_{t}, A_{t+1}, A_{t+2}$…和状态$S_{t}, S_{t+1}, S_{t+2}…$</p><ul><li><p>动作的随机性来自于策略函数，$P[A=a|S=s]=π(a|s)$。</p></li><li><p>状态的随机性来自于状态转移函数，$P[S^{‘}=s^{‘}|S=s,A=a]=p(s^{‘}|s,a)$</p></li></ul><h5 id="动作价值函数和策略π"><a href="#动作价值函数和策略π" class="headerlink" title="动作价值函数和策略π"></a>动作价值函数和策略π</h5><p>$Q_{π}(S_{t}, a_{t})=E[U_{t}|S_{t}, A_{t}=a_{t}]$</p><p>Q值和π和当前的s,a有关，未来的随机性被期望消除了。Qπ只依赖于当前状态和动作，可以反映在当前状态做动作的好坏程度。</p><h5 id="最优动作价值函数"><a href="#最优动作价值函数" class="headerlink" title="最优动作价值函数"></a>最优动作价值函数</h5><p>$Q^{*}(S_{t},a_{t})=max_{π}Q_{π}(s_{t},a_{t})$</p><p>Q*函数基于当前状态st做at的好坏程度，消除了策略π的影响，无论选择何种策略，当前Q都是最好的。</p><h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><p>我们的目标是获取最大收益。</p><p>问题：假设已知$Q*(s,a)$函数，如何选择最好的动作？</p><p>答案：显然最好的动作是使Q值最大的$a*=argmax_{a}Q*(s,a)$，但是我们并不知道$Q*(s,a)$。</p><p>挑战：如何学习出一个Q*函数，可以像先知一样，知道走哪一步最好？</p><p>解决方案：DQN（Deep Q Network）算法，是价值学习方法。使用神经网络$Q(s,a;w)$去近似$Q*(s,a)$函数，神经网络的输入是s，参数是w，输出是很多数值，这些数值是对所有动作的打分，通过奖励来学习神经网络。</p><p><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/1.png" alt="1"></p><p><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/2.png" alt="2"><br>DQN主要学习的是Q函数，神经网络根据输入s，通过Q函数来选择a，然后获得奖励以及新状态…。</p><h2 id="TD"><a href="#TD" class="headerlink" title="TD"></a>TD</h2><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>如图所示，驾车从NYC(纽约)到Atlanta(亚特兰大)，模型Q(w)预测时间为1000分钟，如何更新模型呢？</p><p>预测值$q=Q(w), q=1000$，真实值y=860，损失$L=\frac{1}{2}(q-y)^{2}$，<br>梯度为$\frac{∂L}{∂w}=\frac{∂q}{∂w}·\frac{∂L}{∂q}=(q-y)·\frac{∂Q(w)}{∂w}$，<br>$w_{t+1}=w_{t}-α·\frac{∂L}{∂w}|w=w_{t}$<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/3.png" alt="3"></p><p>这种算法的确定是必须走完全程，才能更新一次参数。TD算法是如何做的呢？</p><ul><li>模型最初的预测$Q(w)=1000$分钟。</li><li>当走了300分钟到达DC时，模型预测600分钟到达Atlanta。更新预测300+600=900分钟。</li><li>TD target900比最初的预测1000分钟更加可靠，因为其中有真实的部分。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/4.png" alt="4"><br>注意到，第一次预测减去第二次预测的值就是TD error。这种方法同样适用于DQN的训练。第一次预测的动作打分是qt，然后前进一步，做出第二次打分yt，yt-qt就是TD error，不断优化这个TD error，模型对动作打分的预测会越来越准。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/5.png" alt="5"></li></ul><p>TD（Temporal Difference Learning）是常用于训练DQN的方法。</p><h5 id="价值学习总结"><a href="#价值学习总结" class="headerlink" title="价值学习总结"></a>价值学习总结</h5><ol><li>观测当前状态$S_{t}=s_{t}$和动作$A_{t}=a_{t}$</li><li>用DQN做一次计算，输入是st，输出是at的打分qt,$q_{t}=Q(s_{t},a_{t};w_{t})$</li><li>用反向传播对DQN求导，得到梯度dt，$dt=\frac{∂Q(s_{t},a_{t};w)}{∂w}|w=w_{t}$</li><li>环境给出新的状态$s_{t+1}$和奖励$r_{t}$</li><li>计算TD target: $y_{t}=r_{t}+γ·max_{a}Q(s_{t+1},a;w_{t})$</li><li>梯度下降，$w_{t+1}=w_{t}-α·(q_{t}-y_{t})·d_{t}$</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之基本概念</title>
      <link href="2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/"/>
      <url>2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/</url>
      
        <content type="html"><![CDATA[<h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>强化学习是一种试错方法，目标是让智能体在特定环境能够采取回报最大化的行为。</p><p>按照学习方式不同，强化学习可以分为：</p><ul><li><p>基于概率（Policy-Based RL）</p><ul><li>policy gradients</li></ul></li><li><p>基于价值（Value-Based RL）</p><ul><li>Q-Learning</li><li>sarsa</li></ul></li></ul><p>按照更新方式不同，强化学习可以分为</p><ul><li><p>回合更新</p><ul><li>基础版policy gradients</li><li>monte-carlo learning</li></ul></li><li><p>单步更新</p><ul><li>Q-Learning</li><li>Sarsa</li><li>升级版Policy Gradients</li></ul></li></ul><p>流行的强化学习方法包括：</p><ul><li>自适应动态规划 ADP</li><li>时间差分学习 TD</li><li>状态-动作-回报-状态-动作算法 SARSA</li><li>Q-Learning</li><li>深度强化学习 DQN</li></ul><p>其主要应用于下棋类、机器人控制和工作调度等。</p><h5 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h5><p>随机变量只取决于随机事件的结果。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/1.png" alt="1"></p><ul><li>X: 随机变量</li><li>x: 观测值</li></ul><p>比如扔了4次硬币，得到4个观测值为</p><ul><li>x1=1 </li><li>x2=0 </li><li>x3=0 </li><li>x4=1。</li></ul><h5 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h5><p>概率密度函数意味着某个随机变量在某个确定的取值点附近的可能性。包括连续分布和离散分布。</p><p>1）高斯分布又叫正态分布，是一个连续分布。<br>$p(x) = \frac{1}{\sqrt{2πσ^{2}}}exp(-\frac{(x-μ)^{2}}{2σ^{2}})$<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/2.png" alt="2"><br>横坐标是随机变量的取值，纵坐标是概率值。该曲线说明，高斯分布的概率密度在原点取值比较大，两边取值比较小。</p><p>2）离散概率分布。</p><p>随机变量X∈{1,3,7}，p(1)=0.2, p(3)=0.5, p(7)=0.3。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/3.png" alt="3"><br>该图说明，X在1、3、7有值，其他概率均为0。</p><h5 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h5><p>若将随机变量X的定义域设为χ，则对于连续分布有$\int_{χ}p(x)dx=1$，对于离散分布有$\sum_{x∈χ}p(x)=1$。</p><p>1）对于连续分布，f(X)的期望为：</p><p>$E[f(X)] = \int_{χ}p(x)·f(x)dx$</p><p>2）对于离散分布，f(X)的期望为：</p><p>$E[f(X)] = \sum_{x∈χ}p(x)·f(x)$</p><h2 id="专业术语"><a href="#专业术语" class="headerlink" title="专业术语"></a>专业术语</h2><h5 id="强化学习-1"><a href="#强化学习-1" class="headerlink" title="强化学习"></a>强化学习</h5><p>强化学习的专业术语包括：state、action、agent、policy。</p><h5 id="agent"><a href="#agent" class="headerlink" title="agent"></a>agent</h5><p>agent做动作的智能体，例如超级玛丽中玛丽就是agent，自动驾驶中汽车是agent。</p><h5 id="state"><a href="#state" class="headerlink" title="state"></a>state</h5><p>state当前agent所处的状态，例如整个界面就是state。</p><h5 id="action"><a href="#action" class="headerlink" title="action"></a>action</h5><p>agent的动作，例如玛丽的上下左右。</p><h5 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h5><p>策略，根据观测的状态来做出决策来控制agent的运动。</p><p>policy函数，$π(s, a)$，范围[0,1]，其中</p><p>$π(s, a) = P(A=a|S=s)$，根据policy函数，可以求出给定状态s，采取动作a的概率。</p><h5 id="reward"><a href="#reward" class="headerlink" title="reward"></a>reward</h5><p>reward，agent获取的奖励。例如对于玛丽而言奖励为R，则</p><ul><li>收集金币 R+=1</li><li>赢得比赛 R += 10000</li><li>碰到敌人 R -= 10000</li><li>什么也没发生 R = 0</li></ul><h5 id="state-transition"><a href="#state-transition" class="headerlink" title="state transition"></a>state transition</h5><p>state transition，状态转移。例如：当前状态玛丽做一个动作，就会产生一个新状态。</p><p>状态转移是随机的，其随机性是从环境中来的。例如玛丽跳起来，敌人可能发射子弹，也可能不发射，而且发射子弹的概率只有环境知道，下一个状态具有随机性。</p><p><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/4.png" alt="4"></p><h5 id="agent和环境交互"><a href="#agent和环境交互" class="headerlink" title="agent和环境交互"></a>agent和环境交互</h5><p><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/5.png" alt="5"><br>例如，agent（玛丽）看到状态（游戏图片）之后，产生动作（上下左右），环境产生一个新的state同时给agent一个奖励。</p><p>(state, action, reward)产生轨迹为s1,a1,r1,s2,a2,r2…sT,aT,rT.</p><h5 id="2个随机性"><a href="#2个随机性" class="headerlink" title="2个随机性"></a>2个随机性</h5><p>强化学习中有2个随机性，这两个随机性对理解强化学习非常重要。<br>一个随机性是来自agengt动作，因为agent动作是根据policy函数随机抽样得到。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/6.png" alt="6"></p><p>另一个随机性来自状态转移（下一个状态）。例如已知当前状态S，并根据policy函数随机抽样产生动作a，系统会随机抽样产生下一个状态。<br>其中，下一个状态有0.8的概率达到一种状态（发射），0.2的概率达到另一种状态。（不发射）<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/7.png" alt="7"></p><h5 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h5><p>Return，回报。未来的累计奖励。</p><p>$U_{t} = R_{t} + R_{t+1} + R_{t+2}…$</p><p>因为未来的奖励没有当前的折扣值钱，所有未来的奖励增加一个折扣奖励的概念。</p><p>$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><h5 id="动作价值函数-Q-s-a"><a href="#动作价值函数-Q-s-a" class="headerlink" title="动作价值函数$Q(s,a)$"></a>动作价值函数$Q(s,a)$</h5><p>事实上，Ut是个随机的变量，无法直接求得，依赖于未来的动作At,At+1,At+2…和状态St,St+1,St+2…。</p><p>如何解决呢？</p><p>Ut未知，St和At为变量且已知它的概率密度函数，则可以通过对Ut求期望的方式求得Qπ。定义Action-value function for policy π。</p><p>$Q_π(S_{t}, a_{t})=E[U_{t}|S_{t}=s_{t},A_{t}=a_{t}]$</p><p>Qπ的直观意义，已知policy函数π，Qπ就会给当前状态下所有的动作打分，然后知道哪个动作好，哪个动作不好。</p><p>那如何选择最好的policy函数π呢？</p><p>可选择policy函数有无数种，最好的policy函数应该是使Q值最大化的那个π。</p><p>$Q^{*}(s_{t}, a_{t})=max_{π}Q_{π}(s_{t}, a_{t})$</p><h5 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h5><p>动作是离散的</p><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\sum_{a}π(a|s_{t})·Q_{π}(s_{t}, a)$</p><p>动作是连续的</p><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\intπ(a|s_{t})·Q_{π}(s_{t}, a)da$</p><p>Vπ可以告诉我们当前的形势好不好。对A求期望可以将A消掉。</p><h5 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h5><p>总计一下，总共有两种价值函数。一种是动作价值函数，一种是状态价值函数。</p><p>动作价值函数，它和policyπ、状态s、动作a有关。它是Ut的条件期望，Ut是未来所有奖励的加权求和，希望把未来的状态和动作都消除，只留下st和at这两个变量。</p><p>评估的是一个智能体在状态s使用动作a是否明智，可以给动作a打分。</p><p>状态价值函数</p><p>状态价值函数，它和policyπ、状态s有关，和动作a无关。</p><p>若使用policyπ，评估当前状态是好是坏。它还可以用来评价policy函数π的好坏，若π越好，Vπ的值越大。</p><h5 id="如何自动打游戏"><a href="#如何自动打游戏" class="headerlink" title="如何自动打游戏"></a>如何自动打游戏</h5><p>一） 学习Policy函数$π(a|s)$</p><ul><li>policy可以根据观测到的状态st做动作，π(·|st)可以给出各个动作的概率，随机采样出动作at。</li></ul><p>二）学习最优动作价值函数$Q^{*}(s,a)$</p><ul><li>观测状态st，选择一个动作可以最大化Q<em>值，Q</em>是未来累计奖励。</li></ul><p>所以，强化学习就是学习π函数或者Q*函数。</p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><ol><li>价值学习</li></ol><ul><li>Deep Q network$(DQN)$，学习$Q*(s,a)$函数。</li><li>如何使用temporal different$(TD)$学习神经网络的参数。</li></ul><ol start="2"><li>策略学习</li></ol><ul><li>policy network，学习$π(a|s)$。</li><li>使用策略梯度学习神经网络的参数。</li></ul><ol start="3"><li><p>Actor-critic method，策略网络和价值网络的结合。</p></li><li><p>AlphaGo的示例。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/03/20/hello-world/"/>
      <url>2021/03/20/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
