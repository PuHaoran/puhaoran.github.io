<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度强化学习实战之价值学习</title>
      <link href="2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/"/>
      <url>2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><h5 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h5><p>回报，未来的累计奖励。</p><p>$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><p>回报取决于动作$A_{t}, A_{t+1}, A_{t+2}$…和状态$S_{t}, S_{t+1}, S_{t+2}…$</p><ul><li><p>动作的随机性来自于策略函数，$P[A=a|S=s]=π(a|s)。</p></li><li><p>状态的随机性来自于状态转移函数，$P[S^{‘}=s^{‘}|S=s,A=a]=p(s^{‘}|s,a)$</p></li></ul><h5 id="动作价值函数和策略π"><a href="#动作价值函数和策略π" class="headerlink" title="动作价值函数和策略π"></a>动作价值函数和策略π</h5><p>$Q_{π}(S_{t}, a_{t})=E[U_{t}|S_{t}, A_{t}=a_{t}]$</p><p>Q值和π和当前的s,a有关，未来的随机性被期望消除了。Qπ只依赖于当前状态和动作，可以反映在当前状态做动作的好坏程度。</p><h5 id="最优动作价值函数"><a href="#最优动作价值函数" class="headerlink" title="最优动作价值函数"></a>最优动作价值函数</h5><p>$Q^{*}(S_{t},a_{t})=max_{π}Q_{π}(s_{t},a_{t})$</p><p>Q*函数基于当前状态st做at的好坏程度，消除了策略π的影响，无论选择何种策略，当前Q都是最好的。</p><h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><p>我们的目标是获取最大收益。</p><p>问题：假设已知Q*(s,a)函数，如何选择最好的动作？</p><p>答案：显然最好的动作是使Q值最大的$a^{<em>}=argmax_{a}Q^{</em>}(s,a)$，但是我们并不知道$Q^{*}(s,a)$。</p><p>挑战：如何学习出一个Q*函数，可以像先知一样，知道走哪一步最好？</p><p>解决方案：DQN（Deep Q Network）算法，是价值学习方法。使用神经网络Q(s,a;w)去近似Q*(s,a)函数，神经网络的输入是s，参数是w，输出是很多数值，这些数值是对所有动作的打分，通过奖励来学习神经网络。</p><p><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/1.png" alt="1"></p><p><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/2.png" alt="2"><br>DQN主要学习的是Q函数，神经网络根据输入s，通过Q函数来选择a，然后获得奖励以及新状态…。</p><h2 id="TD"><a href="#TD" class="headerlink" title="TD"></a>TD</h2><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>如图所示，驾车从NYC(纽约)到Atlanta(亚特兰大)，模型Q(w)预测时间为1000分钟，如何更新模型呢？</p><p>预测值$q=Q(w), q=1000$，真实值y=860，损失$L=\frac{1}{2}(q-y)^{2}$，<br>梯度为$\frac{∂L}{∂w}=\frac{∂q}{∂w}·\frac{∂L}{∂q}=(q-y)·\frac{∂Q(w)}{∂w}$，<br>$w_{t+1}=w_{t}-α·\frac{∂L}{∂w}|w=w_{t}$<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/3.png" alt="3"></p><p>这种算法的确定是必须走完全程，才能更新一次参数。TD算法是如何做的呢？</p><ul><li>模型最初的预测$Q(w)=1000$分钟。</li><li>当走了300分钟到达DC时，模型预测600分钟到达Atlanta。更新预测300+600=900分钟。</li><li>TD target900比最初的预测1000分钟更加可靠，因为其中有真实的部分。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/4.png" alt="4"><br>注意到，第一次预测减去第二次预测的值就是TD error。这种方法同样适用于DQN的训练。第一次预测的动作打分是qt，然后前进一步，做出第二次打分yt，yt-qt就是TD error，不断优化这个TD error，模型对动作打分的预测会越来越准。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/5.png" alt="5"></li></ul><p>TD（Temporal Difference Learning）是常用于训练DQN的方法。</p><ol><li>观测当前状态$S_{t}=s_{t}$和动作$A_{t}=a_{t}$</li><li>用DQN做一次计算，输入是st，输出是at的打分qt,$q_{t}=Q(s_{t},a_{t};w_{t})$</li><li>用反向传播对DQN求导，得到梯度dt，$dt=\frac{∂Q(s_{t},a_{t};w)}{∂w}|w=w_{t}$</li><li>环境给出新的状态$s_{t+1}$和奖励$r_{t}$</li><li>计算TD target: $y_{t}=r_{t}+γ·max_{a}Q(s_{t+1},a;w_{t})$</li><li>梯度下降，$w_{t+1}=w_{t}-α·(q_{t}-y_{t})·d_{t}$</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之基本概念</title>
      <link href="2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/"/>
      <url>2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/</url>
      
        <content type="html"><![CDATA[<h2 id="前提知识"><a href="#前提知识" class="headerlink" title="前提知识"></a>前提知识</h2><h5 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h5><p>随机变量只取决于随机事件的结果。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/1.png" alt="1"></p><ul><li>X: 随机变量</li><li>x: 观测值</li></ul><p>比如扔了4次硬币，得到4个观测值为</p><ul><li>x1=1 </li><li>x2=0 </li><li>x3=0 </li><li>x4=1。</li></ul><h5 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h5><p>概率密度函数意味着某个随机变量在某个确定的取值点附近的可能性。包括连续分布和离散分布。</p><p>1）高斯分布又叫正态分布，是一个连续分布。<br>$p(x) = \frac{1}{\sqrt{2πσ^{2}}}exp(-\frac{(x-μ)^{2}}{2σ^{2}})$<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/2.png" alt="2"><br>横坐标是随机变量的取值，纵坐标是概率值。该曲线说明，高斯分布的概率密度在原点取值比较大，两边取值比较小。</p><p>2）离散概率分布。</p><p>随机变量X∈{1,3,7}，p(1)=0.2, p(3)=0.5, p(7)=0.3。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/3.png" alt="3"><br>该图说明，X在1、3、7有值，其他概率均为0。</p><h5 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h5><p>若将随机变量X的定义域设为χ，则对于连续分布有$\int_{χ}p(x)dx=1$，对于离散分布有$\sum_{x∈χ}p(x)=1$。</p><p>1）对于连续分布，f(X)的期望为：</p><p>$E[f(X)] = \int_{χ}p(x)·f(x)dx$</p><p>2）对于离散分布，f(X)的期望为：</p><p>$E[f(X)] = \sum_{x∈χ}p(x)·f(x)$</p><h2 id="专业术语"><a href="#专业术语" class="headerlink" title="专业术语"></a>专业术语</h2><p>强化学习的专业术语包括：state、action、agent、policy。</p><h5 id="agent"><a href="#agent" class="headerlink" title="agent"></a>agent</h5><p>agent做动作的智能体，例如超级玛丽中玛丽就是agent，自动驾驶中汽车是agent。</p><h5 id="state"><a href="#state" class="headerlink" title="state"></a>state</h5><p>state当前agent所处的状态，例如整个界面就是state。</p><h5 id="action"><a href="#action" class="headerlink" title="action"></a>action</h5><p>agent的动作，例如玛丽的上下左右。</p><h5 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h5><p>策略，根据观测的状态来做出决策来控制agent的运动。</p><p>policy函数，$π(s, a)$，范围[0,1]，其中</p><p>$π(s, a) = P(A=a|S=s)$，根据policy函数，可以求出给定状态s，采取动作a的概率。</p><h5 id="reward"><a href="#reward" class="headerlink" title="reward"></a>reward</h5><p>reward，agent获取的奖励。例如对于玛丽而言奖励为R，则</p><ul><li>收集金币 R+=1</li><li>赢得比赛 R += 10000</li><li>碰到敌人 R -= 10000</li><li>什么也没发生 R = 0</li></ul><h5 id="state-transition"><a href="#state-transition" class="headerlink" title="state transition"></a>state transition</h5><p>state transition，状态转移。例如：当前状态玛丽做一个动作，就会产生一个新状态。</p><p>状态转移是随机的，其随机性是从环境中来的。例如玛丽跳起来，敌人可能发射子弹，也可能不发射，而且发射子弹的概率只有环境知道，下一个状态具有随机性。</p><p><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/4.png" alt="4"></p><h5 id="agent和环境交互"><a href="#agent和环境交互" class="headerlink" title="agent和环境交互"></a>agent和环境交互</h5><p><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/5.png" alt="5"><br>例如，agent（玛丽）看到状态（游戏图片）之后，产生动作（上下左右），环境产生一个新的state同时给agent一个奖励。</p><p>(state, action, reward)产生轨迹为s1,a1,r1,s2,a2,r2…sT,aT,rT.</p><h5 id="2个随机性"><a href="#2个随机性" class="headerlink" title="2个随机性"></a>2个随机性</h5><p>强化学习中有2个随机性，这两个随机性对理解强化学习非常重要。<br>一个随机性是来自agengt动作，因为agent动作是根据policy函数随机抽样得到。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/6.png" alt="6"></p><p>另一个随机性来自状态转移（下一个状态）。例如已知当前状态S，并根据policy函数随机抽样产生动作a，系统会随机抽样产生下一个状态。<br>其中，下一个状态有0.8的概率达到一种状态（发射），0.2的概率达到另一种状态。（不发射）<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/7.png" alt="7"></p><h5 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h5><p>Return，回报。未来的累计奖励。</p><p>$U_{t} = R_{t} + R_{t+1} + R_{t+2}…$</p><p>因为未来的奖励没有当前的折扣值钱，所有未来的奖励增加一个折扣奖励的概念。</p><p>$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><h5 id="动作价值函数-Q-s-a"><a href="#动作价值函数-Q-s-a" class="headerlink" title="动作价值函数$Q(s,a)$"></a>动作价值函数$Q(s,a)$</h5><p>事实上，Ut是个随机的变量，无法直接求得，依赖于未来的动作At,At+1,At+2…和状态St,St+1,St+2…。</p><p>如何解决呢？</p><p>Ut未知，St和At为变量且已知它的概率密度函数，则可以通过对Ut求期望的方式求得Qπ。定义Action-value function for policy π。</p><p>$Q_π(S_{t}, a_{t})=E[U_{t}|S_{t}=s_{t},A_{t}=a_{t}]$</p><p>Qπ的直观意义，已知policy函数π，Qπ就会给当前状态下所有的动作打分，然后知道哪个动作好，哪个动作不好。</p><p>那如何选择最好的policy函数π呢？</p><p>可选择policy函数有无数种，最好的policy函数应该是使Q值最大化的那个π。</p><p>$Q^{*}(s_{t}, a_{t})=max_{π}Q_{π}(s_{t}, a_{t})$</p><h5 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h5><p>动作是离散的</p><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\sum_{a}π(a|s_{t})·Q_{π}(s_{t}, a)$</p><p>动作是连续的</p><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\intπ(a|s_{t})·Q_{π}(s_{t}, a)da$</p><p>Vπ可以告诉我们当前的形势好不好。对A求期望可以将A消掉。</p><h5 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h5><p>总计一下，总共有两种价值函数。一种是动作价值函数，一种是状态价值函数。</p><p>动作价值函数，它和policyπ、状态s、动作a有关。它是Ut的条件期望，Ut是未来所有奖励的加权求和，希望把未来的状态和动作都消除，只留下st和at这两个变量。</p><p>评估的是一个智能体在状态s使用动作a是否明智，可以给动作a打分。</p><p>状态价值函数</p><p>状态价值函数，它和policyπ、状态s有关，和动作a无关。</p><p>若使用policyπ，评估当前状态是好是坏。它还可以用来评价policy函数π的好坏，若π越好，Vπ的值越大。</p><h5 id="如何自动打游戏"><a href="#如何自动打游戏" class="headerlink" title="如何自动打游戏"></a>如何自动打游戏</h5><p>一） 学习Policy函数$π(a|s)$</p><ul><li>policy可以根据观测到的状态st做动作，π(·|st)可以给出各个动作的概率，随机采样出动作at。</li></ul><p>二）学习最优动作价值函数$Q^{*}(s,a)$</p><ul><li>观测状态st，选择一个动作可以最大化Q<em>值，Q</em>是未来累计奖励。</li></ul><p>所以，强化学习就是学习π函数或者Q*函数。</p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><ol><li>价值学习</li></ol><ul><li>Deep Q network$(DQN)$，学习$Q*(s,a)$函数。</li><li>如何使用temporal different$(TD)$学习神经网络的参数。</li></ul><ol start="2"><li>策略学习</li></ol><ul><li>policy network，学习$π(a|s)$。</li><li>使用策略梯度学习神经网络的参数。</li></ul><ol start="3"><li><p>Actor-critic method，策略网络和价值网络的结合。</p></li><li><p>AlphaGo的示例。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/03/20/hello-world/"/>
      <url>2021/03/20/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
