<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度强化学习实战之Q-learning</title>
      <link href="2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/"/>
      <url>2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/</url>
      
        <content type="html"><![CDATA[<p>Q-Learning，是另一种TD算法，DQN就是Q-learning的一种实现方式。</p><h2 id="Sarsa-vs-Q-Learning"><a href="#Sarsa-vs-Q-Learning" class="headerlink" title="Sarsa vs Q-Learning"></a>Sarsa vs Q-Learning</h2><p>sarsa和Qlearning都是TD算法，但解决的问题不同。</p><h5 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h5><p>Sarsa是训练价值动作函数，$Q_{π}(s,a)$</p><p>TD target: $y_{t} = r_{t} + γ·Q_{π}(s_{t+1}, a_{t+1})$</p><p>我们使用sarsa更新价值网络(critic)。</p><h5 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h5><p>Q-learning是训练最优动作价值函数$Q^{*}(s,a)$</p><p>TD target: $y_{t} = r_{t} + γ·max_{a}Q^{*}(s_{t+1}, a)$</p><p>我们使用Q-learning更新DQN。</p><h2 id="推导TD-target"><a href="#推导TD-target" class="headerlink" title="推导TD target"></a>推导TD target</h2><p>$Q_{π}(s_{t}, a_{t})=E[R_{t}+γ·Q_{π}(S_{t+1}, A_{t+1})]$</p><p>如果π是最优策略$π^{*}$</p><p>$Q_{π*}(s_{t}, a_{t})=E[R_{t}+γ·Q_{π^{*}}(S_{t+1}, A_{t+1})]$</p><p>$Q_{π*}$和$Q^{*}$都表示最优动作价值函数。</p><p>$Q*(s_{t},a_{t})=E[R_{t}+γ·Q*(S_{t+1},A_{t+1})]$</p><p>动作$A_{t+1}$可以计算为</p><p>$A_{t+1}=argmax_{a}Q*(S_{t+1},a)$</p><p>所以，$Q*(s_{t},a_{t})=E[R_{t}+γ·max_{a}Q*(S_{t+1},a)]$</p><p>直接求期望困难，所以做蒙特卡洛近似，期望中有$R_{t}$用$r_{t}$做近似，$S_{t+1}$用$s_{t}$近似。</p><p>$Q*(s_{t},a_{t})=E[R_{t}+γ·max_{a}Q*(S_{t+1},a)]≈r_{t}+γ·max_{a}Q*(s_{t+1},a)$ (TD target)</p><h2 id="Q-Learning的表格形式"><a href="#Q-Learning的表格形式" class="headerlink" title="Q-Learning的表格形式"></a>Q-Learning的表格形式</h2><h5 id="Q-Learning的表格形式-1"><a href="#Q-Learning的表格形式-1" class="headerlink" title="Q-Learning的表格形式"></a>Q-Learning的表格形式</h5><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/1.png" alt="1"></p><h5 id="学习流程"><a href="#学习流程" class="headerlink" title="学习流程"></a>学习流程</h5><ol><li>观测四元组$(s_{t}, a_{t}, r_{t}, s_{t+1})$</li><li>TD target: $y_{t} = r_{t} + γ·max_{a}Q^{*}(s_{t+1}, a)$</li><li>TD error: $σ_{t}=Q^{*}(s_{t}, a_{t})-y_{t}$</li><li>更新参数: $Q*(s_{t}, a_{t})&lt;-Q*(s_{t}, a_{t})-α·σ_{t}$</li></ol><h2 id="Q-Learning价值网络形式"><a href="#Q-Learning价值网络形式" class="headerlink" title="Q-Learning价值网络形式"></a>Q-Learning价值网络形式</h2><h5 id="Q-Learning价值网络形式-1"><a href="#Q-Learning价值网络形式-1" class="headerlink" title="Q-Learning价值网络形式"></a>Q-Learning价值网络形式</h5><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-q-learning/2.png" alt="2"></p><h2 id="Q-Learning总结"><a href="#Q-Learning总结" class="headerlink" title="Q-Learning总结"></a>Q-Learning总结</h2><p>目标：学习最优动作价值函数$Q^{*}$<br>形式：</p><ul><li><p>表格</p><ul><li>直接学习$Q*$</li><li>有限的状态和动作</li><li>绘制表格，通过Q-learning更新表格的动作价值</li></ul></li><li><p>价值神经网络</p><ul><li>根据DQN$Q(s,a;w)$近似$Q*$</li><li>使用Q-learning更新参数w</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之sarsa</title>
      <link href="2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/"/>
      <url>2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/</url>
      
        <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Sarsa是TD算法的一种。算法每次用一个五元组$(s_{t},a_{t},r_{t},s_{t+1},a_{t+1})$来更新$Q_{π}$，全称State-action-reward-state-action，简称SARSA。</p><h2 id="TD-target推导"><a href="#TD-target推导" class="headerlink" title="TD target推导"></a>TD target推导</h2><p>当前时刻的回报$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/1.png" alt="1"><br>得出，$U_{t}=R_{t} + γ·U_{t+1}$。</p><p>其中，$R_{t}$依赖于$(S_{t}, A_{t}, S_{t+1})$</p><p>$Q_{π}(s_{t},a_{t})=E[U_{t}|s_{t},a_{t}]$</p><p>= $E[R_{t}+γ·U_{t+1}|s_{t},a_{t}]$</p><p>= $E[R_{t}|s_{t},a_{t}]+γ·E[U_{t+1}|s_{t},a_{t}]$</p><p>= $E[R_{t}|s_{t},a_{t}]+γ·E[Q_{π}(S_{t+1},A_{t+1})|s_{t},a_{t}]$<br><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/2.png" alt="2"></p><p>目标是使动作价值$Q_{π}$更接近$y_{t}$。</p><h2 id="Sarsa表格形式"><a href="#Sarsa表格形式" class="headerlink" title="Sarsa表格形式"></a>Sarsa表格形式</h2><h5 id="Sars的表格形式"><a href="#Sars的表格形式" class="headerlink" title="Sars的表格形式"></a>Sars的表格形式</h5><ul><li>我们想要学习$Q_{π}(s,a)$</li><li>状态和动作都是有限的。</li><li>表中每个元素对应一个动作价值，每次更新表格中的一个元素。</li></ul><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/3.png" alt="3"></p><h5 id="学习流程"><a href="#学习流程" class="headerlink" title="学习流程"></a>学习流程</h5><ol><li>每次观测一个四元组$(s_{t},a_{t},r_{t},s_{t+1})$</li><li>根据policy函数π和$s_{t+1}$随机抽样一个$a_{t+1}$</li><li>TD target: $y_{t}=r_{t}+γ·[Q_{π}(s_{t+1},a_{t+1})$</li><li>TD error: $σ_{t}=Q_{π}(s_{t},a_{t})-y_{t}$</li><li>更新动作价值Q: $σ_{t}(s_{t}, a_{t})&lt;-Q_{π}(s_{t},a_{t})-α·σ_{t}$</li></ol><h2 id="Sarsa价值网络形式"><a href="#Sarsa价值网络形式" class="headerlink" title="Sarsa价值网络形式"></a>Sarsa价值网络形式</h2><h5 id="Sarsa价值网络形式-1"><a href="#Sarsa价值网络形式-1" class="headerlink" title="Sarsa价值网络形式"></a>Sarsa价值网络形式</h5><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/4.png" alt="4"></p><ul><li>通过价值网络q(s,a;w)近似$Q_{π}(s,a)$</li><li>q作为一个评论家来评估演员的好坏。（Actor-Critic方法）</li><li>学习参数w。</li></ul><h5 id="学习流程-1"><a href="#学习流程-1" class="headerlink" title="学习流程"></a>学习流程</h5><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-sarsa/5.png" alt="5"></p><h2 id="Sarsa总结"><a href="#Sarsa总结" class="headerlink" title="Sarsa总结"></a>Sarsa总结</h2><p>目标：学习动作价值函数$Q_{π}$<br>形式：</p><ul><li><p>表格</p><ul><li>直接学习Qπ</li><li>有限的状态和动作</li><li>绘制表格，通过sarsa更新表格的动作价值</li></ul></li><li><p>价值神经网络</p><ul><li>根据价值网络$q(s,a;w)$近似$Q_{π}$</li><li>使用sarsa更新参数</li><li>应用：actor-critic方法</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之Actor-Critic</title>
      <link href="2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/"/>
      <url>2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/</url>
      
        <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Actor-Critic合并了策略学习和价值学习两种强化学习算法。Actor是策略网络，用来控制agent运动，可以看做演员；Critic是价值网络，用来给动作打分，可以看做评论家员。</p><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/1.png" alt="1"></p><h5 id="价值网络和策略网络"><a href="#价值网络和策略网络" class="headerlink" title="价值网络和策略网络"></a>价值网络和策略网络</h5><p>状态价值函数</p><p>$V_{π}=\sum_{a}π(a|s)·Q_{π}(s,a)≈\sum_{a}π(a|s;θ)·Q_{π}(s,a;w)$</p><p>公式中π和$Q_{π}$都是未知的。</p><p>策略网络(actor)</p><ul><li>使用神经网络$π(a|s;θ)$去近似$π(a|s)$</li><li>神经网络训练参数θ</li></ul><p>价值网络(critic)</p><ul><li>使用神经网络$q(s,a;w)$去近似$Q_{π}(s,a)$</li><li>神经网络训练参数w</li></ul><p>actor是个演员可以做动作，但他不知道什么是好的动作，这时候就需要评论家critic来评判。学习这两个网络的目的是让演员的平均分越来越高，同时让评论家的打分越来越准。</p><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/2.png" alt="2"></p><p><img src="/2021/03/30/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-actor-critic/3.png" alt="3"></p><h5 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h5><p>使用神经网络近似这两个函数</p><p>$V(s;θ,w)=\sum_{a}π(a|s;θ)·q(s,a;w)$</p><p>更新参数θ和w</p><p>更新策略网络$π(a|s;θ)$为了增加状态值$V(s;θ,w)$。</p><p>更新价值网络q(s,a;w)为了打分越来越精准。</p><ol><li>观测状态$s_{t}$。</li><li>随机抽样根据$π(·|s_{t};θ_{t})$得到动作$a_{t}$。</li><li>完成$a_{t}$并观测新的状态$s_{t+1}$和奖励$r_{t}$。</li><li>更新w(价值网络参数)使用时间差异(TD)。</li><li>更新θ(策略网络参数)使用策略梯度。</li></ol><h5 id="使用TD算法更新价值网络q"><a href="#使用TD算法更新价值网络q" class="headerlink" title="使用TD算法更新价值网络q"></a>使用TD算法更新价值网络q</h5><ul><li>计算$q(s_{t},a_{t};w_{t})$和$q(s_{t+1},a_{t+1};w_{t})$</li><li>TD target: $y_{t}=r_{t}+γ·q(s_{t+1},a_{t+1};w_{t})$</li><li>损失: $L(w)=\frac{1}{2}[q(s_{t},a_{t};w)-y_{t}]^{2}$</li><li>梯度下降: $w_{t+1}=w_{t}-α·\frac{∂L(w)}{∂w}|w=w_{t}$</li></ul><p>使得评论家打分更准。</p><h5 id="使用策略梯度更新策略网络π"><a href="#使用策略梯度更新策略网络π" class="headerlink" title="使用策略梯度更新策略网络π"></a>使用策略梯度更新策略网络π</h5><ul><li><p>神经网络近似状态价值函数$V(s;θ,w)=\sum_{a}π(a|s;θ)·q(s,a;w)$</p></li><li><p>策略梯度，更新$V(s_{t};θ,w)$的θ</p></li></ul><p>$g(a,θ)=\frac{∂logπ(a|s,θ)}{∂θ}·q(s_{t},a;w)$</p><p>$\frac{∂V(s;θ,w_{t})}{∂θ}=E_{A}[g(A, θ)]$</p><p>根据π函数随机采样a，$θ_{t+1}=θ_{t}+β·g(a,θ_{t})$。</p><p>使得演员得分更高。</p><h5 id="Actor-Critic总结"><a href="#Actor-Critic总结" class="headerlink" title="Actor-Critic总结"></a>Actor-Critic总结</h5><ol><li>观测状态点$s_{t}$和随机抽样$a_{t}$~$π(·|s_{t};θ_{t})$</li><li>完成$a_{t}$，环境给一个新的状态点$s_{t+1}$和回报$r_{t}$</li><li>随机抽样$a_{t+1}^{-}$~$π(·|s_{t};θ_{t})$，注意并未执行$a_{t+1}^{-}$</li><li>评估价值网络$q_{t}=q(s_{t},a_{t};w_{t})$和$q_{t+1}=q(s_{t+1},a_{t+1}^{-};w_{t})$</li><li>计算TD error: $σ_{t}=q_{t}-(r_{t}+γ·q_{t+1})$</li><li>价值网络求导: $d_{w,t}=\frac{∂q(s_{t},a_{t};w)}{∂w}|w=w_{t}$</li><li>更新价值网络: $w_{t+1}=w_{t}-α·σ_{t}·d_{w,t}$</li><li>策略网络求导: $d_{θ,t}=\frac{∂logπ(a_{t}|s_{t};θ)}{∂θ}|θ=θ_{t}$</li><li>更新策略网络: $θ_{t+1}=θ_{t}+β·q_{t}·d_{θ,t}$</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之策略学习</title>
      <link href="2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/"/>
      <url>2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="策略学习"><a href="#策略学习" class="headerlink" title="策略学习"></a>策略学习</h2><h5 id="policy函数"><a href="#policy函数" class="headerlink" title="policy函数"></a>policy函数</h5><p>策略，根据观测的状态来做出决策来控制agent的运动。</p><p>policy函数，$π(s, a)$，范围[0,1]，其中</p><p>$π(s, a) = P(A=a|S=s)$，根据policy函数，可以求出给定状态s，采取动作a的概率。</p><p>理想中，我们可以采用$s × a$的表代表policy的概率，但对于超级玛丽这种游戏，有无数种状态，需要用一个policy网络来近似。</p><h5 id="policy-network"><a href="#policy-network" class="headerlink" title="policy network"></a>policy network</h5><p>策略网络（policy network）$π(a|s;θ)$近似$π(a|s)$。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/1.png" alt="1"></p><p>$\sum_{a∈A}(a|s;θ)=1$，其中A={‘left’, ‘right’, ‘up’}，是一个动作集合。</p><h5 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h5><p>return，当前和未来的累计奖励</p><p>$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><h5 id="动作值函数"><a href="#动作值函数" class="headerlink" title="动作值函数"></a>动作值函数</h5><p>$Q_π(S_{t}, a_{t})=E[U_{t}|S_{t}=s_{t},A_{t}=a_{t}]$</p><h5 id="状态值函数"><a href="#状态值函数" class="headerlink" title="状态值函数"></a>状态值函数</h5><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\sum_{a}π(a|s_{t})·Q_{π}(s_{t}, a)$</p><p><strong>近似状态值函数</strong></p><p>$V_{π}(s_{t};θ)=E_{A}[Q_{π}(s_{t}, A)]=\sum_{a}π(a|s_{t};θ)·Q_{π}(s_{t}, a)$</p><p>目标是最大化状态价值的期望，$J(θ)=E_{S}[V(S;θ)]$</p><p>如何更新θ呢？</p><p>$θ &lt;- θ + β·\frac{∂V(s;θ)}{∂θ}$</p><h5 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h5><ul><li>第一种形式</li></ul><p>$\frac{∂V(s;θ)}{∂θ}=\frac{∂\sum_{a}π(a|s;θ)·Q_{π}(s,a)}{∂θ}$</p><p>= $\sum_{a}\frac{∂π(a|s;θ)·Q_{π}(s,a)}{∂θ}$</p><p>= $\sum_{a}\frac{∂π(a|s;θ)}{∂θ}·Q_{π}(s,a)$ // 假设θ是独立于$Q_{π}$。</p><ul><li>第二种形式</li></ul><p>$\frac{∂V(s;θ)}{∂θ}$ = $\sum_{a}\frac{∂π(a|s;θ)}{∂θ}·Q_{π}(s,a)$</p><p>= $\sum_{a}π(a|s;θ)·\frac{∂logπ(a|s;θ)}{∂θ}$</p><p>这两种形式是等价的，离散的使用第一种，连续的使用第二种。</p><p>对于离散的动作，我们取第一种。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/2.png" alt="2"></p><p>对于连续的动作，可以有[0,1]所有的实数，动作有无数种，无法做动作的加和，故使用第二个公式。<br>先通过policyπ函数随机抽样一个a，然后根据蒙特卡洛近似出g，使用g近似策略梯度。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ce-lue-xue-xi/3.png" alt="3"></p><h5 id="策略梯度算法总结"><a href="#策略梯度算法总结" class="headerlink" title="策略梯度算法总结"></a>策略梯度算法总结</h5><ol><li>观测到$s_t$</li><li>策略网络随机抽样动作$a_t$</li><li>计算t时刻的动作价值$q_{t}≈Q_{π}(s_{t}, a_{t})$</li><li>对策略网络求导$d_{θ,t}=\frac{∂logπ(a_{t}|s_{t};θ)}{∂θ}|θ=θ_{t}$</li><li>近似策略梯度$g(a_{t},θ_{t})=q_{t}·d_{θ,t}$</li><li>更新策略网络$θ_{t+1}=θ_{t}+β·g(a_{t},θ_{t})$</li></ol><h5 id="如何计算-Q-π-s-t-a-t"><a href="#如何计算-Q-π-s-t-a-t" class="headerlink" title="如何计算$Q_{π}(s_{t},a_{t})$"></a>如何计算$Q_{π}(s_{t},a_{t})$</h5><p>方式一：增强(reinforce)</p><p>用策略网络π控制小球运动，从开始玩到游戏结束。s1,a1,r1,s2,a2,r2…sT,aT,rT，然后计算所有奖励r的加权求和。</p><p>计算所有r的$u_{t}=\sum_{k=t}^{T}r^{k-t}r_{k}$</p><p>因为$Qπ(s_{t},a_{t})=E[U_{t}]$，我们使用$u_{t}$近似$Qπ(s_{t},a_{t})$。</p><p>方式二：使用神经网络近似$Q_{π}$</p><p>这样就有了两个网络，一个用来近似π，一个用来近似$Q_{π}$</p><h2 id="策略学习总结"><a href="#策略学习总结" class="headerlink" title="策略学习总结"></a>策略学习总结</h2><ol><li>我们希望得到策略函数π，然后用π自动控制agent运动。每当agent观察st，agent就用π自动算出at。</li><li>直接求策略函数比较困难，故用神经网络近似策略函数，这个网络称为策略网络(policy network)$π(a|s;θ)$。</li><li>根据策略梯度学习策略网络。</li><li>策略梯度是状态价值函数V关于θ的导数$E_{S}[V(S;θ)]$。（注：目标函数是状态价值V关于状态的期望）</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之价值学习</title>
      <link href="2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/"/>
      <url>2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><h5 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h5><p>回报，未来的累计奖励。</p><p>$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><p>回报取决于动作$A_{t}, A_{t+1}, A_{t+2}$…和状态$S_{t}, S_{t+1}, S_{t+2}…$</p><ul><li><p>动作的随机性来自于策略函数，$P[A=a|S=s]=π(a|s)$。</p></li><li><p>状态的随机性来自于状态转移函数，$P[S^{‘}=s^{‘}|S=s,A=a]=p(s^{‘}|s,a)$</p></li></ul><h5 id="动作价值函数和策略π"><a href="#动作价值函数和策略π" class="headerlink" title="动作价值函数和策略π"></a>动作价值函数和策略π</h5><p>$Q_{π}(S_{t}, a_{t})=E[U_{t}|S_{t}, A_{t}=a_{t}]$</p><p>Q值和π和当前的s,a有关，未来的随机性被期望消除了。Qπ只依赖于当前状态和动作，可以反映在当前状态做动作的好坏程度。</p><h5 id="最优动作价值函数"><a href="#最优动作价值函数" class="headerlink" title="最优动作价值函数"></a>最优动作价值函数</h5><p>$Q^{*}(S_{t},a_{t})=max_{π}Q_{π}(s_{t},a_{t})$</p><p>Q*函数基于当前状态st做at的好坏程度，消除了策略π的影响，无论选择何种策略，当前Q都是最好的。</p><h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><p>我们的目标是获取最大收益。</p><p>问题：假设已知$Q*(s,a)$函数，如何选择最好的动作？</p><p>答案：显然最好的动作是使Q值最大的$a^{<em>}=argmax_{a}Q^{</em>}(s,a)$，但是我们并不知道$Q^{*}(s,a)$。</p><p>挑战：如何学习出一个Q*函数，可以像先知一样，知道走哪一步最好？</p><p>解决方案：DQN（Deep Q Network）算法，是价值学习方法。使用神经网络Q(s,a;w)去近似Q*(s,a)函数，神经网络的输入是s，参数是w，输出是很多数值，这些数值是对所有动作的打分，通过奖励来学习神经网络。</p><p><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/1.png" alt="1"></p><p><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/2.png" alt="2"><br>DQN主要学习的是Q函数，神经网络根据输入s，通过Q函数来选择a，然后获得奖励以及新状态…。</p><h2 id="TD"><a href="#TD" class="headerlink" title="TD"></a>TD</h2><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p>如图所示，驾车从NYC(纽约)到Atlanta(亚特兰大)，模型Q(w)预测时间为1000分钟，如何更新模型呢？</p><p>预测值$q=Q(w), q=1000$，真实值y=860，损失$L=\frac{1}{2}(q-y)^{2}$，<br>梯度为$\frac{∂L}{∂w}=\frac{∂q}{∂w}·\frac{∂L}{∂q}=(q-y)·\frac{∂Q(w)}{∂w}$，<br>$w_{t+1}=w_{t}-α·\frac{∂L}{∂w}|w=w_{t}$<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/3.png" alt="3"></p><p>这种算法的确定是必须走完全程，才能更新一次参数。TD算法是如何做的呢？</p><ul><li>模型最初的预测$Q(w)=1000$分钟。</li><li>当走了300分钟到达DC时，模型预测600分钟到达Atlanta。更新预测300+600=900分钟。</li><li>TD target900比最初的预测1000分钟更加可靠，因为其中有真实的部分。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/4.png" alt="4"><br>注意到，第一次预测减去第二次预测的值就是TD error。这种方法同样适用于DQN的训练。第一次预测的动作打分是qt，然后前进一步，做出第二次打分yt，yt-qt就是TD error，不断优化这个TD error，模型对动作打分的预测会越来越准。<br><img src="/2021/03/29/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-jie-zhi-xue-xi/5.png" alt="5"></li></ul><p>TD（Temporal Difference Learning）是常用于训练DQN的方法。</p><h5 id="价值学习总结"><a href="#价值学习总结" class="headerlink" title="价值学习总结"></a>价值学习总结</h5><ol><li>观测当前状态$S_{t}=s_{t}$和动作$A_{t}=a_{t}$</li><li>用DQN做一次计算，输入是st，输出是at的打分qt,$q_{t}=Q(s_{t},a_{t};w_{t})$</li><li>用反向传播对DQN求导，得到梯度dt，$dt=\frac{∂Q(s_{t},a_{t};w)}{∂w}|w=w_{t}$</li><li>环境给出新的状态$s_{t+1}$和奖励$r_{t}$</li><li>计算TD target: $y_{t}=r_{t}+γ·max_{a}Q(s_{t+1},a;w_{t})$</li><li>梯度下降，$w_{t+1}=w_{t}-α·(q_{t}-y_{t})·d_{t}$</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之基本概念</title>
      <link href="2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/"/>
      <url>2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/</url>
      
        <content type="html"><![CDATA[<h2 id="前提知识"><a href="#前提知识" class="headerlink" title="前提知识"></a>前提知识</h2><h5 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h5><p>随机变量只取决于随机事件的结果。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/1.png" alt="1"></p><ul><li>X: 随机变量</li><li>x: 观测值</li></ul><p>比如扔了4次硬币，得到4个观测值为</p><ul><li>x1=1 </li><li>x2=0 </li><li>x3=0 </li><li>x4=1。</li></ul><h5 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h5><p>概率密度函数意味着某个随机变量在某个确定的取值点附近的可能性。包括连续分布和离散分布。</p><p>1）高斯分布又叫正态分布，是一个连续分布。<br>$p(x) = \frac{1}{\sqrt{2πσ^{2}}}exp(-\frac{(x-μ)^{2}}{2σ^{2}})$<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/2.png" alt="2"><br>横坐标是随机变量的取值，纵坐标是概率值。该曲线说明，高斯分布的概率密度在原点取值比较大，两边取值比较小。</p><p>2）离散概率分布。</p><p>随机变量X∈{1,3,7}，p(1)=0.2, p(3)=0.5, p(7)=0.3。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/3.png" alt="3"><br>该图说明，X在1、3、7有值，其他概率均为0。</p><h5 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h5><p>若将随机变量X的定义域设为χ，则对于连续分布有$\int_{χ}p(x)dx=1$，对于离散分布有$\sum_{x∈χ}p(x)=1$。</p><p>1）对于连续分布，f(X)的期望为：</p><p>$E[f(X)] = \int_{χ}p(x)·f(x)dx$</p><p>2）对于离散分布，f(X)的期望为：</p><p>$E[f(X)] = \sum_{x∈χ}p(x)·f(x)$</p><h2 id="专业术语"><a href="#专业术语" class="headerlink" title="专业术语"></a>专业术语</h2><h5 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h5><p>强化学习是一种试错方法，目标是让智能体在特定环境能够采取回报最大化的行为。流行的强化学习方法包括：</p><ul><li>自适应动态规划 ADP</li><li>时间差分学习 TD</li><li>状态-动作-回报-状态-动作算法 SARSA</li><li>Q学习</li><li>深度强化学习 DQN</li></ul><p>其主要应用于下棋类、机器人控制和工作调度等。</p><p>强化学习的专业术语包括：state、action、agent、policy。</p><h5 id="agent"><a href="#agent" class="headerlink" title="agent"></a>agent</h5><p>agent做动作的智能体，例如超级玛丽中玛丽就是agent，自动驾驶中汽车是agent。</p><h5 id="state"><a href="#state" class="headerlink" title="state"></a>state</h5><p>state当前agent所处的状态，例如整个界面就是state。</p><h5 id="action"><a href="#action" class="headerlink" title="action"></a>action</h5><p>agent的动作，例如玛丽的上下左右。</p><h5 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h5><p>策略，根据观测的状态来做出决策来控制agent的运动。</p><p>policy函数，$π(s, a)$，范围[0,1]，其中</p><p>$π(s, a) = P(A=a|S=s)$，根据policy函数，可以求出给定状态s，采取动作a的概率。</p><h5 id="reward"><a href="#reward" class="headerlink" title="reward"></a>reward</h5><p>reward，agent获取的奖励。例如对于玛丽而言奖励为R，则</p><ul><li>收集金币 R+=1</li><li>赢得比赛 R += 10000</li><li>碰到敌人 R -= 10000</li><li>什么也没发生 R = 0</li></ul><h5 id="state-transition"><a href="#state-transition" class="headerlink" title="state transition"></a>state transition</h5><p>state transition，状态转移。例如：当前状态玛丽做一个动作，就会产生一个新状态。</p><p>状态转移是随机的，其随机性是从环境中来的。例如玛丽跳起来，敌人可能发射子弹，也可能不发射，而且发射子弹的概率只有环境知道，下一个状态具有随机性。</p><p><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/4.png" alt="4"></p><h5 id="agent和环境交互"><a href="#agent和环境交互" class="headerlink" title="agent和环境交互"></a>agent和环境交互</h5><p><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/5.png" alt="5"><br>例如，agent（玛丽）看到状态（游戏图片）之后，产生动作（上下左右），环境产生一个新的state同时给agent一个奖励。</p><p>(state, action, reward)产生轨迹为s1,a1,r1,s2,a2,r2…sT,aT,rT.</p><h5 id="2个随机性"><a href="#2个随机性" class="headerlink" title="2个随机性"></a>2个随机性</h5><p>强化学习中有2个随机性，这两个随机性对理解强化学习非常重要。<br>一个随机性是来自agengt动作，因为agent动作是根据policy函数随机抽样得到。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/6.png" alt="6"></p><p>另一个随机性来自状态转移（下一个状态）。例如已知当前状态S，并根据policy函数随机抽样产生动作a，系统会随机抽样产生下一个状态。<br>其中，下一个状态有0.8的概率达到一种状态（发射），0.2的概率达到另一种状态。（不发射）<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/7.png" alt="7"></p><h5 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h5><p>Return，回报。未来的累计奖励。</p><p>$U_{t} = R_{t} + R_{t+1} + R_{t+2}…$</p><p>因为未来的奖励没有当前的折扣值钱，所有未来的奖励增加一个折扣奖励的概念。</p><p>$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><h5 id="动作价值函数-Q-s-a"><a href="#动作价值函数-Q-s-a" class="headerlink" title="动作价值函数$Q(s,a)$"></a>动作价值函数$Q(s,a)$</h5><p>事实上，Ut是个随机的变量，无法直接求得，依赖于未来的动作At,At+1,At+2…和状态St,St+1,St+2…。</p><p>如何解决呢？</p><p>Ut未知，St和At为变量且已知它的概率密度函数，则可以通过对Ut求期望的方式求得Qπ。定义Action-value function for policy π。</p><p>$Q_π(S_{t}, a_{t})=E[U_{t}|S_{t}=s_{t},A_{t}=a_{t}]$</p><p>Qπ的直观意义，已知policy函数π，Qπ就会给当前状态下所有的动作打分，然后知道哪个动作好，哪个动作不好。</p><p>那如何选择最好的policy函数π呢？</p><p>可选择policy函数有无数种，最好的policy函数应该是使Q值最大化的那个π。</p><p>$Q^{*}(s_{t}, a_{t})=max_{π}Q_{π}(s_{t}, a_{t})$</p><h5 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h5><p>动作是离散的</p><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\sum_{a}π(a|s_{t})·Q_{π}(s_{t}, a)$</p><p>动作是连续的</p><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\intπ(a|s_{t})·Q_{π}(s_{t}, a)da$</p><p>Vπ可以告诉我们当前的形势好不好。对A求期望可以将A消掉。</p><h5 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h5><p>总计一下，总共有两种价值函数。一种是动作价值函数，一种是状态价值函数。</p><p>动作价值函数，它和policyπ、状态s、动作a有关。它是Ut的条件期望，Ut是未来所有奖励的加权求和，希望把未来的状态和动作都消除，只留下st和at这两个变量。</p><p>评估的是一个智能体在状态s使用动作a是否明智，可以给动作a打分。</p><p>状态价值函数</p><p>状态价值函数，它和policyπ、状态s有关，和动作a无关。</p><p>若使用policyπ，评估当前状态是好是坏。它还可以用来评价policy函数π的好坏，若π越好，Vπ的值越大。</p><h5 id="如何自动打游戏"><a href="#如何自动打游戏" class="headerlink" title="如何自动打游戏"></a>如何自动打游戏</h5><p>一） 学习Policy函数$π(a|s)$</p><ul><li>policy可以根据观测到的状态st做动作，π(·|st)可以给出各个动作的概率，随机采样出动作at。</li></ul><p>二）学习最优动作价值函数$Q^{*}(s,a)$</p><ul><li>观测状态st，选择一个动作可以最大化Q<em>值，Q</em>是未来累计奖励。</li></ul><p>所以，强化学习就是学习π函数或者Q*函数。</p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><ol><li>价值学习</li></ol><ul><li>Deep Q network$(DQN)$，学习$Q*(s,a)$函数。</li><li>如何使用temporal different$(TD)$学习神经网络的参数。</li></ul><ol start="2"><li>策略学习</li></ol><ul><li>policy network，学习$π(a|s)$。</li><li>使用策略梯度学习神经网络的参数。</li></ul><ol start="3"><li><p>Actor-critic method，策略网络和价值网络的结合。</p></li><li><p>AlphaGo的示例。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度强化学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/03/20/hello-world/"/>
      <url>2021/03/20/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
