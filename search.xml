<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>test</title>
      <link href="2021/03/28/test/"/>
      <url>2021/03/28/test/</url>
      
        <content type="html"><![CDATA[<p>test</p><p><img src="/2021/03/28/test/1.png" alt="1"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深度强化学习实战之基本概念</title>
      <link href="2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/"/>
      <url>2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/</url>
      
        <content type="html"><![CDATA[<h2 id="前提知识"><a href="#前提知识" class="headerlink" title="前提知识"></a>前提知识</h2><h5 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h5><p>随机变量只取决于随机事件的结果。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/1.png" alt="1"></p><ul><li>X: 随机变量</li><li>x: 观测值</li></ul><p>比如扔了4次硬币，得到4个观测值为</p><ul><li>x1=1 </li><li>x2=0 </li><li>x3=0 </li><li>x4=1。</li></ul><h5 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h5><p>概率密度函数意味着某个随机变量在某个确定的取值点附近的可能性。包括连续分布和离散分布。</p><p>1）高斯分布又叫正态分布，是一个连续分布。<br>$p(x) = \frac{1}{\sqrt{2πσ^{2}}}exp(-\frac{(x-μ)^{2}}{2σ^{2}})$<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/2.png" alt="2"><br>横坐标是随机变量的取值，纵坐标是概率值。该曲线说明，高斯分布的概率密度在原点取值比较大，两边取值比较小。</p><p>2）离散概率分布。</p><p>随机变量X∈{1,3,7}，p(1)=0.2, p(3)=0.5, p(7)=0.3。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/3.png" alt="3"><br>该图说明，X在1、3、7有值，其他概率均为0。</p><h5 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h5><p>若将随机变量X的定义域设为χ，则对于连续分布有$\int_{χ}p(x)dx=1$，对于离散分布有$\sum_{x∈χ}p(x)=1$。</p><p>1）对于连续分布，f(X)的期望为：</p><p>$E[f(X)] = \int_{χ}p(x)·f(x)dx$</p><p>2）对于离散分布，f(X)的期望为：</p><p>$E[f(X)] = \sum_{x∈χ}p(x)·f(x)$</p><h2 id="专业术语"><a href="#专业术语" class="headerlink" title="专业术语"></a>专业术语</h2><p>强化学习的专业术语包括：state、action、agent、policy。</p><h5 id="agent"><a href="#agent" class="headerlink" title="agent"></a>agent</h5><p>agent做动作的智能体，例如超级玛丽中玛丽就是agent，自动驾驶中汽车是agent。</p><h5 id="state"><a href="#state" class="headerlink" title="state"></a>state</h5><p>state当前agent所处的状态，例如整个界面就是state。</p><h5 id="action"><a href="#action" class="headerlink" title="action"></a>action</h5><p>agent的动作，例如玛丽的上下左右。</p><h5 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h5><p>策略，根据观测的状态来做出决策来控制agent的运动。</p><p>policy函数，$π(s, a)$，范围[0,1]，其中</p><p>$π(s, a) = P(A=a|S=s)$，根据policy函数，可以求出给定状态s，采取动作a的概率。</p><h5 id="reward"><a href="#reward" class="headerlink" title="reward"></a>reward</h5><p>reward，agent获取的奖励。例如对于玛丽而言奖励为R，则</p><ul><li>收集金币 R+=1</li><li>赢得比赛 R += 10000</li><li>碰到敌人 R -= 10000</li><li>什么也没发生 R = 0</li></ul><h5 id="state-transition"><a href="#state-transition" class="headerlink" title="state transition"></a>state transition</h5><p>state transition，状态转移。例如：当前状态玛丽做一个动作，就会产生一个新状态。</p><p>状态转移是随机的，其随机性是从环境中来的。例如玛丽跳起来，敌人可能发射子弹，也可能不发射，而且发射子弹的概率只有环境知道，下一个状态具有随机性。</p><p><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/4.png" alt="4"></p><h5 id="agent和环境交互"><a href="#agent和环境交互" class="headerlink" title="agent和环境交互"></a>agent和环境交互</h5><p><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/5.png" alt="5"><br>例如，agent（玛丽）看到状态（游戏图片）之后，产生动作（上下左右），环境产生一个新的state同时给agent一个奖励。</p><p>(state, action, reward)产生轨迹为s1,a1,r1,s2,a2,r2…sT,aT,rT.</p><h5 id="2个随机性"><a href="#2个随机性" class="headerlink" title="2个随机性"></a>2个随机性</h5><p>强化学习中有2个随机性，这两个随机性对理解强化学习非常重要。<br>一个随机性是来自agengt动作，因为agent动作是根据policy函数随机抽样得到。<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/6.png" alt="6"></p><p>另一个随机性来自状态转移（下一个状态）。例如已知当前状态S，并根据policy函数随机抽样产生动作a，系统会随机抽样产生下一个状态。<br>其中，下一个状态有0.8的概率达到一种状态（发射），0.2的概率达到另一种状态。（不发射）<br><img src="/2021/03/28/shen-du-qiang-hua-xue-xi-shi-zhan-zhi-ji-ben-gai-nian/7.png" alt="7"></p><h5 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h5><p>Return，回报。未来的累计奖励。</p><p>$U_{t} = R_{t} + R_{t+1} + R_{t+2}…$</p><p>因为未来的奖励没有当前的折扣值钱，所有未来的奖励增加一个折扣奖励的概念。</p><p>$U_{t} = R_{t} + γR_{t+1} + γ^{2}R_{t+2}…$</p><h5 id="动作价值函数-Q-s-a"><a href="#动作价值函数-Q-s-a" class="headerlink" title="动作价值函数$Q(s,a)$"></a>动作价值函数$Q(s,a)$</h5><p>事实上，Ut是个随机的变量，无法直接求得，依赖于未来的动作At,At+1,At+2…和状态St,St+1,St+2…。</p><p>如何解决呢？</p><p>Ut未知，St和At为变量且已知它的概率密度函数，则可以通过对Ut求期望的方式求得Qπ。定义Action-value function for policy π。</p><p>$Q_π(S_{t}, a_{t})=E[U_{t}|S_{t}=s_{t},A_{t}=a_{t}]$</p><p>Qπ的直观意义，已知policy函数π，Qπ就会给当前状态下所有的动作打分，然后知道哪个动作好，哪个动作不好。</p><p>那如何选择最好的policy函数π呢？</p><p>可选择policy函数有无数种，最好的policy函数应该是使Q值最大化的那个π。</p><p>$Q^{*}(s_{t}, a_{t})=max_{π}Q_{π}(s_{t}, a_{t})$</p><h5 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h5><p>动作是离散的</p><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\sum_{a}π(a|s_{t})·Q_{π}(s_{t}, a)$</p><p>动作是连续的</p><p>$V_{π}(s_{t})=E_{A}[Q_{π}(s_{t}, A)]=\intπ(a|s_{t})·Q_{π}(s_{t}, a)da$</p><p>Vπ可以告诉我们当前的形势好不好。对A求期望可以将A消掉。</p><h5 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h5><p>总计一下，总共有两种价值函数。一种是动作价值函数，一种是状态价值函数。</p><p>动作价值函数，它和policyπ、状态s、动作a有关。它是Ut的条件期望，Ut是未来所有奖励的加权求和，希望把未来的状态和动作都消除，只留下st和at这两个变量。</p><p>评估的是一个智能体在状态s使用动作a是否明智，可以给动作a打分。</p><p>状态价值函数</p><p>状态价值函数，它和policyπ、状态s有关，和动作a无关。</p><p>若使用policyπ，评估当前状态是好是坏。它还可以用来评价policy函数π的好坏，若π越好，Vπ的值越大。</p><h5 id="如何自动打游戏"><a href="#如何自动打游戏" class="headerlink" title="如何自动打游戏"></a>如何自动打游戏</h5><p>一） 学习Policy函数$π(a|s)$</p><ul><li>policy可以根据观测到的状态st做动作，π(·|st)可以给出各个动作的概率，随机采样出动作at。</li></ul><p>二）学习最优动作价值函数$Q^{*}(s,a)$</p><ul><li>观测状态st，选择一个动作可以最大化Q<em>值，Q</em>是未来累计奖励。</li></ul><p>所以，强化学习就是学习π函数或者Q*函数。</p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><ol><li>价值学习</li></ol><ul><li>Deep Q network(DQN)，学习$Q*(s,a)$函数。</li><li>使用temporal different(TD)学习神经网络的参数。</li></ul><ol start="2"><li>策略学习</li></ol><ul><li>policy network，学习$π(a|s)$。</li><li>使用策略梯度学习神经网络的参数。</li></ul><ol start="3"><li><p>Actor-critic method，策略网络和价值网络的结合。</p></li><li><p>AlphaGo的示例。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/03/20/hello-world/"/>
      <url>2021/03/20/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
